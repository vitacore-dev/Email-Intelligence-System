#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
üöÄ –£–ª—É—á—à–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è –∞–ª–≥–æ—Ä–∏—Ç–º–∞ —Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏—è ORCID v3.0

–ö–ª—é—á–µ–≤—ã–µ —É–ª—É—á—à–µ–Ω–∏—è:
- –ú–∞—à–∏–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –¥–ª—è –∞–¥–∞–ø—Ç–∏–≤–Ω—ã—Ö –≤–µ—Å–æ–≤
- –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑ –∏–º–µ–Ω —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º transformers
- –í—Ä–µ–º–µ–Ω–Ω–æ–π –∞–Ω–∞–ª–∏–∑ –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ ORCID –ø—Ä–æ—Ñ–∏–ª–µ–π
- –ì—Ä–∞—Ñ —Å–≤—è–∑–µ–π –º–µ–∂–¥—É –∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—è–º–∏
- –ú—É–ª—å—Ç–∏–ª–∏–Ω–≥–≤–∞–ª—å–Ω–∞—è –ø–æ–¥–¥–µ—Ä–∂–∫–∞
- –ê–¥–∞–ø—Ç–∏–≤–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –≤–µ—Å–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
- –ê–Ω–∞–ª–∏–∑ —Ü–∏—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –∏ –∏–º–ø–∞–∫—Ç-—Ñ–∞–∫—Ç–æ—Ä–∞
- –°–∏—Å—Ç–µ–º–∞ –æ–±—Ä–∞—Ç–Ω–æ–π —Å–≤—è–∑–∏ –¥–ª—è —Å–∞–º–æ–æ–±—É—á–µ–Ω–∏—è

–ê–≤—Ç–æ—Ä: AI Assistant
–î–∞—Ç–∞: 2025-07-04
–í–µ—Ä—Å–∏—è: 3.0
"""

import re
import json
import logging
import numpy as np
from typing import Dict, List, Optional, Any, Tuple
from datetime import datetime, timedelta
import requests
from dataclasses import dataclass, asdict
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from sentence_transformers import SentenceTransformer
import networkx as nx
from collections import defaultdict, Counter
import hashlib
import sqlite3
import pickle
from pathlib import Path

logger = logging.getLogger(__name__)

@dataclass
class ORCIDCandidate:
    """–°—Ç—Ä—É–∫—Ç—É—Ä–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∫–∞–Ω–¥–∏–¥–∞—Ç–∞ ORCID"""
    orcid_id: str
    url: str
    position_in_search: int
    relevance_score: float = 0.0
    confidence_level: str = "low"
    
    # –§–∞–∫—Ç–æ—Ä—ã —Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏—è
    position_score: float = 0.0
    url_quality_score: float = 0.0
    name_similarity_score: float = 0.0
    domain_quality_score: float = 0.0
    domain_affinity_score: float = 0.0
    temporal_score: float = 0.0
    network_score: float = 0.0
    citation_score: float = 0.0
    semantic_score: float = 0.0
    
    # –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
    extracted_names: List[str] = None
    publication_count: int = 0
    last_activity: Optional[datetime] = None
    research_areas: List[str] = None
    h_index: Optional[int] = None
    
    def __post_init__(self):
        if self.extracted_names is None:
            self.extracted_names = []
        if self.research_areas is None:
            self.research_areas = []

class EnhancedORCIDRankingAlgorithm:
    """–£–ª—É—á—à–µ–Ω–Ω—ã–π –∞–ª–≥–æ—Ä–∏—Ç–º —Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏—è ORCID —Å ML –∏ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–º –∞–Ω–∞–ª–∏–∑–æ–º"""
    
    def __init__(self, cache_dir: str = "./cache", enable_ml: bool = True):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∞–ª–≥–æ—Ä–∏—Ç–º–∞
        
        Args:
            cache_dir: –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–ª—è –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏—è –º–æ–¥–µ–ª–µ–π –∏ –¥–∞–Ω–Ω—ã—Ö
            enable_ml: –í–∫–ª—é—á–∏—Ç—å —Ñ—É–Ω–∫—Ü–∏–∏ –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è
        """
        self.cache_dir = Path(cache_dir)
        self.cache_dir.mkdir(exist_ok=True)
        self.enable_ml = enable_ml
        
        # –ë–∞–∑–æ–≤—ã–µ –≤–µ—Å–∞ (–º–æ–≥—É—Ç –∞–¥–∞–ø—Ç–∏—Ä–æ–≤–∞—Ç—å—Å—è)
        self.base_weights = {
            'position': 0.12,           # –°–Ω–∏–∂–µ–Ω –¥–ª—è —Ñ–æ–∫—É—Å–∞ –Ω–∞ —Å–µ–º–∞–Ω—Ç–∏–∫–µ
            'url_quality': 0.18,        # –ö–∞—á–µ—Å—Ç–≤–æ URL
            'name_similarity': 0.35,    # –£–≤–µ–ª–∏—á–µ–Ω - –æ—Å–Ω–æ–≤–Ω–æ–π —Ñ–∞–∫—Ç–æ—Ä
            'domain_quality': 0.12,     # –ö–∞—á–µ—Å—Ç–≤–æ –¥–æ–º–µ–Ω–∞
            'domain_affinity': 0.08,    # –°–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ –¥–æ–º–µ–Ω–∞
            'temporal': 0.05,           # –í—Ä–µ–º–µ–Ω–Ω–æ–π —Ñ–∞–∫—Ç–æ—Ä (–Ω–æ–≤—ã–π)
            'network': 0.05,            # –°–µ—Ç–µ–≤–æ–π –∞–Ω–∞–ª–∏–∑ (–Ω–æ–≤—ã–π)
            'citation': 0.03,           # –¶–∏—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ (–Ω–æ–≤—ã–π)
            'semantic': 0.02            # –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑ (–Ω–æ–≤—ã–π)
        }
        
        # –ö–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã–µ –º–æ–¥–∏—Ñ–∏–∫–∞—Ç–æ—Ä—ã –≤–µ—Å–æ–≤
        self.context_weight_modifiers = {
            'academic': {'name_similarity': 1.2, 'citation': 1.5, 'temporal': 1.3},
            'corporate': {'domain_affinity': 1.4, 'url_quality': 1.2},
            'personal': {'name_similarity': 1.5, 'semantic': 1.3}
        }
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è ML –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤
        self._init_ml_components()
        
        # –ì—Ä–∞—Ñ —Å–≤—è–∑–µ–π –º–µ–∂–¥—É –∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—è–º–∏
        self.researcher_graph = nx.Graph()
        
        # –ë–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
        self._init_feedback_db()
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —Ä–∞–±–æ—Ç—ã –∞–ª–≥–æ—Ä–∏—Ç–º–∞
        self.stats = {
            'total_processed': 0,
            'successful_matches': 0,
            'feedback_received': 0,
            'accuracy_history': []
        }
    
    def _init_ml_components(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤ –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è"""
        try:
            if self.enable_ml:
                # –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∞—è –º–æ–¥–µ–ª—å –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –∏–º–µ–Ω
                model_path = self.cache_dir / "sentence_transformer_model"
                if model_path.exists():
                    logger.info("–ó–∞–≥—Ä—É–∂–∞–µ–º —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω—É—é –º–æ–¥–µ–ª—å SentenceTransformer")
                    self.semantic_model = SentenceTransformer(str(model_path))
                else:
                    logger.info("–ó–∞–≥—Ä—É–∂–∞–µ–º –Ω–æ–≤—É—é –º–æ–¥–µ–ª—å SentenceTransformer")
                    self.semantic_model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')
                    self.semantic_model.save(str(model_path))
                
                # TF-IDF –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ç–æ—Ä –¥–ª—è —Ç–µ–∫—Å—Ç–æ–≤–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞
                self.tfidf_vectorizer = TfidfVectorizer(
                    max_features=1000,
                    stop_words=None,  # –ü–æ–¥–¥–µ—Ä–∂–∫–∞ –º—É–ª—å—Ç–∏—è–∑—ã—á–Ω–æ—Å—Ç–∏
                    lowercase=True,
                    ngram_range=(1, 2)
                )
                
                logger.info("ML –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã —É—Å–ø–µ—à–Ω–æ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω—ã")
            else:
                self.semantic_model = None
                self.tfidf_vectorizer = None
                logger.info("ML –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã –æ—Ç–∫–ª—é—á–µ–Ω—ã")
                
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ ML –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤: {str(e)}")
            self.semantic_model = None
            self.tfidf_vectorizer = None
    
    def _init_feedback_db(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—Ä–∞—Ç–Ω–æ–π —Å–≤—è–∑–∏"""
        db_path = self.cache_dir / "feedback.db"
        self.db_conn = sqlite3.connect(str(db_path), check_same_thread=False)
        
        # –°–æ–∑–¥–∞–µ–º —Ç–∞–±–ª–∏—Ü—ã –µ—Å–ª–∏ –∏—Ö –Ω–µ—Ç
        self.db_conn.execute("""
            CREATE TABLE IF NOT EXISTS feedback (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                email TEXT,
                selected_orcid TEXT,
                correct_orcid TEXT,
                confidence REAL,
                factors TEXT,
                timestamp DATETIME DEFAULT CURRENT_TIMESTAMP
            )
        """)
        
        self.db_conn.execute("""
            CREATE TABLE IF NOT EXISTS algorithm_performance (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                version TEXT,
                accuracy REAL,
                precision_score REAL,
                recall_score REAL,
                f1_score REAL,
                test_cases_count INTEGER,
                timestamp DATETIME DEFAULT CURRENT_TIMESTAMP
            )
        """)
        
        self.db_conn.commit()
    
    def rank_orcid_candidates(self, 
                            candidates: List[Dict[str, Any]], 
                            email: str,
                            context: str = "academic",
                            target_name: Optional[str] = None) -> List[ORCIDCandidate]:
        """
        –ì–ª–∞–≤–Ω—ã–π –º–µ—Ç–æ–¥ —Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏—è –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤ ORCID
        
        Args:
            candidates: –°–ø–∏—Å–æ–∫ –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤ —Å –±–∞–∑–æ–≤–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π
            email: Email –¥–ª—è –ø–æ–∏—Å–∫–∞
            context: –ö–æ–Ω—Ç–µ–∫—Å—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è (academic, corporate, personal)
            target_name: –ò–∑–≤–µ—Å—Ç–Ω–æ–µ –∏–º—è –≤–ª–∞–¥–µ–ª—å—Ü–∞ email (–µ—Å–ª–∏ –µ—Å—Ç—å)
        
        Returns:
            –û—Ç—Å–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Å–ø–∏—Å–æ–∫ ORCIDCandidate
        """
        logger.info(f"üéØ –ù–∞—á–∏–Ω–∞–µ–º —É–ª—É—á—à–µ–Ω–Ω–æ–µ —Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏–µ {len(candidates)} ORCID –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤")
        logger.info(f"üìß Email: {email}")
        logger.info(f"üè¢ –ö–æ–Ω—Ç–µ–∫—Å—Ç: {context}")
        logger.info(f"üë§ –¶–µ–ª–µ–≤–æ–µ –∏–º—è: {target_name or '–Ω–µ —É–∫–∞–∑–∞–Ω–æ'}")
        
        # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –æ–±—ä–µ–∫—Ç—ã
        structured_candidates = []
        for i, candidate in enumerate(candidates):
            orcid_candidate = ORCIDCandidate(
                orcid_id=candidate.get('orcid', candidate.get('orcid_id', '')),
                url=candidate.get('url', ''),
                position_in_search=candidate.get('position_in_list', i)
            )
            structured_candidates.append(orcid_candidate)
        
        # –ê–¥–∞–ø—Ç–∏—Ä—É–µ–º –≤–µ—Å–∞ –ø–æ–¥ –∫–æ–Ω—Ç–µ–∫—Å—Ç
        current_weights = self._adapt_weights_for_context(context)
        logger.info(f"üìä –ê–¥–∞–ø—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –≤–µ—Å–∞ –¥–ª—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ '{context}': {current_weights}")
        
        # –û–±–æ–≥–∞—â–∞–µ–º –¥–∞–Ω–Ω—ã–º–∏ –∏–∑ ORCID API
        enriched_candidates = self._enrich_candidates_with_orcid_data(structured_candidates)
        
        # –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º —Ñ–∞–∫—Ç–æ—Ä—ã —Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏—è
        for candidate in enriched_candidates:
            self._calculate_all_ranking_factors(candidate, email, target_name, current_weights)
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –æ–±—â–µ–π —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏
        ranked_candidates = sorted(enriched_candidates, 
                                 key=lambda x: x.relevance_score, 
                                 reverse=True)
        
        # –ü—Ä–∏–º–µ–Ω—è–µ–º –ø–æ—Å—Ç–ø—Ä–æ—Ü–µ—Å—Å–∏–Ω–≥
        final_candidates = self._post_process_ranking(ranked_candidates, context)
        
        # –û–±–Ω–æ–≤–ª—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
        self.stats['total_processed'] += 1
        
        # –õ–æ–≥–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        self._log_ranking_results(final_candidates, email)
        
        return final_candidates
    
    def _adapt_weights_for_context(self, context: str) -> Dict[str, float]:
        """–ê–¥–∞–ø—Ç–∏—Ä—É–µ—Ç –≤–µ—Å–∞ —Ñ–∞–∫—Ç–æ—Ä–æ–≤ –ø–æ–¥ –∫–æ–Ω—Ç–µ–∫—Å—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è"""
        weights = self.base_weights.copy()
        
        if context in self.context_weight_modifiers:
            modifiers = self.context_weight_modifiers[context]
            for factor, modifier in modifiers.items():
                if factor in weights:
                    weights[factor] *= modifier
        
        # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –≤–µ—Å–∞
        total_weight = sum(weights.values())
        if total_weight != 1.0:
            for factor in weights:
                weights[factor] /= total_weight
        
        return weights
    
    def _enrich_candidates_with_orcid_data(self, candidates: List[ORCIDCandidate]) -> List[ORCIDCandidate]:
        """–û–±–æ–≥–∞—â–∞–µ—Ç –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤ –¥–∞–Ω–Ω—ã–º–∏ –∏–∑ ORCID API"""
        logger.info(f"üîç –û–±–æ–≥–∞—â–∞–µ–º {len(candidates)} –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤ –¥–∞–Ω–Ω—ã–º–∏ –∏–∑ ORCID API...")
        
        enriched = []
        for candidate in candidates:
            try:
                # –ü–æ–ª—É—á–∞–µ–º –ø—Ä–æ—Ñ–∏–ª—å –∏–∑ ORCID
                profile_data = self._fetch_orcid_profile(candidate.orcid_id)
                
                if profile_data:
                    # –ò–∑–≤–ª–µ–∫–∞–µ–º –∏–º–µ–Ω–∞
                    candidate.extracted_names = self._extract_names_from_profile(profile_data)
                    
                    # –ò–∑–≤–ª–µ–∫–∞–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
                    candidate.publication_count = self._count_publications(profile_data)
                    candidate.last_activity = self._get_last_activity(profile_data)
                    candidate.research_areas = self._extract_research_areas(profile_data)
                    candidate.h_index = self._estimate_h_index(profile_data)
                    
                    logger.info(f"‚úÖ –û–±–æ–≥–∞—â–µ–Ω ORCID {candidate.orcid_id}: {len(candidate.extracted_names)} –∏–º–µ–Ω, {candidate.publication_count} –ø—É–±–ª–∏–∫–∞—Ü–∏–π")
                else:
                    logger.warning(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –¥–∞–Ω–Ω—ã–µ –¥–ª—è ORCID {candidate.orcid_id}")
                
                enriched.append(candidate)
                
            except Exception as e:
                logger.error(f"‚ùå –û—à–∏–±–∫–∞ –æ–±–æ–≥–∞—â–µ–Ω–∏—è ORCID {candidate.orcid_id}: {str(e)}")
                enriched.append(candidate)
        
        return enriched
    
    def _fetch_orcid_profile(self, orcid_id: str) -> Optional[Dict]:
        """–ü–æ–ª—É—á–∞–µ—Ç –ø—Ä–æ—Ñ–∏–ª—å ORCID (–∑–∞–≥–ª—É—à–∫–∞ –¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏)"""
        # –í —Ä–µ–∞–ª—å–Ω–æ–π —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏ –∑–¥–µ—Å—å –±—É–¥–µ—Ç –∑–∞–ø—Ä–æ—Å –∫ ORCID API
        # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –∑–∞–≥–ª—É—à–∫—É –¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏
        return {
            'person': {
                'name': {
                    'given-names': {'value': 'John'},
                    'family-name': {'value': 'Doe'}
                }
            },
            'activities-summary': {
                'works': {'group': [{'work-summary': [{}]}] * 5}
            }
        }
    
    def _extract_names_from_profile(self, profile: Dict) -> List[str]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —Ä–∞–∑–ª–∏—á–Ω—ã–µ –≤–∞—Ä–∏–∞–Ω—Ç—ã –∏–º–µ–Ω –∏–∑ –ø—Ä–æ—Ñ–∏–ª—è ORCID"""
        names = []
        
        # –û—Å–Ω–æ–≤–Ω–æ–µ –∏–º—è
        person = profile.get('person', {})
        name_data = person.get('name', {})
        
        given_names = name_data.get('given-names', {}).get('value', '')
        family_name = name_data.get('family-name', {}).get('value', '')
        
        if given_names and family_name:
            names.extend([
                f"{given_names} {family_name}",
                f"{family_name} {given_names}",
                f"{given_names[0]}. {family_name}" if given_names else family_name
            ])
        
        # –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–µ –∏–º–µ–Ω–∞
        other_names = person.get('other-names', {}).get('other-name', [])
        for other_name in other_names:
            if isinstance(other_name, dict) and 'content' in other_name:
                names.append(other_name['content'])
        
        # –ö—Ä–µ–¥–∏—Ç–Ω–æ–µ –∏–º—è
        credit_name = name_data.get('credit-name', {}).get('value', '')
        if credit_name:
            names.append(credit_name)
        
        return list(set(names))  # –£–±–∏—Ä–∞–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã
    
    def _count_publications(self, profile: Dict) -> int:
        """–ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ—Ç –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—É–±–ª–∏–∫–∞—Ü–∏–π"""
        works = profile.get('activities-summary', {}).get('works', {}).get('group', [])
        return len(works)
    
    def _get_last_activity(self, profile: Dict) -> Optional[datetime]:
        """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç –¥–∞—Ç—É –ø–æ—Å–ª–µ–¥–Ω–µ–π –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏"""
        # –ó–∞–≥–ª—É—à–∫–∞ - –≤ —Ä–µ–∞–ª—å–Ω–æ—Å—Ç–∏ –∏–∑–≤–ª–µ–∫–∞–µ—Ç—Å—è –∏–∑ –ø—Ä–æ—Ñ–∏–ª—è
        return datetime.now() - timedelta(days=30)
    
    def _extract_research_areas(self, profile: Dict) -> List[str]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –æ–±–ª–∞—Å—Ç–∏ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–π"""
        # –ó–∞–≥–ª—É—à–∫–∞ - –≤ —Ä–µ–∞–ª—å–Ω–æ—Å—Ç–∏ –∏–∑–≤–ª–µ–∫–∞–µ—Ç—Å—è –∏–∑ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ –∏ —Ä–∞–±–æ—Ç
        return ["Computer Science", "Machine Learning", "Data Analysis"]
    
    def _estimate_h_index(self, profile: Dict) -> Optional[int]:
        """–û—Ü–µ–Ω–∏–≤–∞–µ—Ç h-index –∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—è"""
        # –ó–∞–≥–ª—É—à–∫–∞ - –≤ —Ä–µ–∞–ª—å–Ω–æ—Å—Ç–∏ —Ä–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ—Ç—Å—è –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ü–∏—Ç–∏—Ä–æ–≤–∞–Ω–∏–π
        return 15
    
    def _calculate_all_ranking_factors(self, 
                                     candidate: ORCIDCandidate, 
                                     email: str, 
                                     target_name: Optional[str],
                                     weights: Dict[str, float]):
        """–†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ—Ç –≤—Å–µ —Ñ–∞–∫—Ç–æ—Ä—ã —Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏—è –¥–ª—è –∫–∞–Ω–¥–∏–¥–∞—Ç–∞"""
        
        # 1. –§–∞–∫—Ç–æ—Ä –ø–æ–∑–∏—Ü–∏–∏ –≤ –ø–æ–∏—Å–∫–µ
        candidate.position_score = self._calculate_position_score(candidate.position_in_search)
        
        # 2. –ö–∞—á–µ—Å—Ç–≤–æ URL
        candidate.url_quality_score = self._calculate_url_quality_score(candidate.url)
        
        # 3. –°—Ö–æ–¥—Å—Ç–≤–æ –∏–º–µ–Ω (–æ—Å–Ω–æ–≤–Ω–æ–π —Ñ–∞–∫—Ç–æ—Ä)
        candidate.name_similarity_score = self._calculate_name_similarity_score(
            candidate.extracted_names, target_name, email
        )
        
        # 4. –ö–∞—á–µ—Å—Ç–≤–æ –¥–æ–º–µ–Ω–∞
        candidate.domain_quality_score = self._calculate_domain_quality_score(candidate.url)
        
        # 5. –î–æ–º–µ–Ω–Ω–∞—è –ø—Ä–∏–Ω–∞–¥–ª–µ–∂–Ω–æ—Å—Ç—å
        candidate.domain_affinity_score = self._calculate_domain_affinity_score(
            candidate.url, email
        )
        
        # 6. –í—Ä–µ–º–µ–Ω–Ω–æ–π —Ñ–∞–∫—Ç–æ—Ä (–Ω–æ–≤—ã–π)
        candidate.temporal_score = self._calculate_temporal_score(candidate.last_activity)
        
        # 7. –°–µ—Ç–µ–≤–æ–π –∞–Ω–∞–ª–∏–∑ (–Ω–æ–≤—ã–π)
        candidate.network_score = self._calculate_network_score(candidate.orcid_id)
        
        # 8. –§–∞–∫—Ç–æ—Ä —Ü–∏—Ç–∏—Ä–æ–≤–∞–Ω–∏—è (–Ω–æ–≤—ã–π)
        candidate.citation_score = self._calculate_citation_score(
            candidate.h_index, candidate.publication_count
        )
        
        # 9. –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑ (–Ω–æ–≤—ã–π)
        candidate.semantic_score = self._calculate_semantic_score(
            candidate.research_areas, email, target_name
        )
        
        # –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º –æ–±—â—É—é —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å
        candidate.relevance_score = (
            candidate.position_score * weights['position'] +
            candidate.url_quality_score * weights['url_quality'] +
            candidate.name_similarity_score * weights['name_similarity'] +
            candidate.domain_quality_score * weights['domain_quality'] +
            candidate.domain_affinity_score * weights['domain_affinity'] +
            candidate.temporal_score * weights['temporal'] +
            candidate.network_score * weights['network'] +
            candidate.citation_score * weights['citation'] +
            candidate.semantic_score * weights['semantic']
        )
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —É—Ä–æ–≤–µ–Ω—å —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏
        candidate.confidence_level = self._determine_confidence_level(candidate.relevance_score)
        
        logger.info(f"üéØ ORCID {candidate.orcid_id}: —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å = {candidate.relevance_score:.3f} "
                   f"(—É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {candidate.confidence_level})")
    
    def _calculate_position_score(self, position: int) -> float:
        """–†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ—Ç –æ—Ü–µ–Ω–∫—É –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø–æ–∑–∏—Ü–∏–∏ –≤ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞—Ö –ø–æ–∏—Å–∫–∞"""
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ª–æ–≥–∞—Ä–∏—Ñ–º–∏—á–µ—Å–∫–æ–µ —É–±—ã–≤–∞–Ω–∏–µ –≤–º–µ—Å—Ç–æ –ª–∏–Ω–µ–π–Ω–æ–≥–æ
        return max(0, 1.0 - (np.log(position + 1) / np.log(101)))
    
    def _calculate_url_quality_score(self, url: str) -> float:
        """–†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ—Ç –∫–∞—á–µ—Å—Ç–≤–æ URL"""
        score = 0.0
        
        if not url:
            return 0.0
        
        url_lower = url.lower()
        
        # –ü—Ä—è–º–∞—è —Å—Å—ã–ª–∫–∞ –Ω–∞ ORCID
        if 'orcid.org' in url_lower:
            score += 0.8
        
        # HTTPS
        if url.startswith('https://'):
            score += 0.1
        
        # –û—Ç—Å—É—Ç—Å—Ç–≤–∏–µ –ø–æ–¥–æ–∑—Ä–∏—Ç–µ–ª—å–Ω—ã—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
        if url.count('?') <= 1 and url.count('&') <= 3:
            score += 0.1
        
        return min(1.0, score)
    
    def _calculate_name_similarity_score(self, 
                                       extracted_names: List[str], 
                                       target_name: Optional[str], 
                                       email: str) -> float:
        """–£–ª—É—á—à–µ–Ω–Ω—ã–π —Ä–∞—Å—á–µ—Ç —Å—Ö–æ–¥—Å—Ç–≤–∞ –∏–º–µ–Ω —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º ML"""
        if not extracted_names:
            return 0.0
        
        max_score = 0.0
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –≤–æ–∑–º–æ–∂–Ω–æ–µ –∏–º—è –∏–∑ email
        email_name_part = email.split('@')[0]
        potential_names = [target_name] if target_name else []
        potential_names.append(email_name_part)
        
        for potential_name in potential_names:
            if not potential_name:
                continue
                
            for extracted_name in extracted_names:
                # –¢—Ä–∞–¥–∏—Ü–∏–æ–Ω–Ω—ã–µ –º–µ—Ç–æ–¥—ã
                traditional_score = self._calculate_traditional_name_similarity(
                    potential_name, extracted_name
                )
                
                # –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ —Å ML (–µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–Ω–æ)
                semantic_score = 0.0
                if self.semantic_model:
                    try:
                        semantic_score = self._calculate_semantic_name_similarity(
                            potential_name, extracted_name
                        )
                    except Exception as e:
                        logger.warning(f"–û—à–∏–±–∫–∞ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞: {str(e)}")
                
                # –ö–æ–º–±–∏–Ω–∏—Ä—É–µ–º –æ—Ü–µ–Ω–∫–∏
                combined_score = 0.7 * traditional_score + 0.3 * semantic_score
                max_score = max(max_score, combined_score)
        
        return min(1.0, max_score)
    
    def _calculate_traditional_name_similarity(self, name1: str, name2: str) -> float:
        """–¢—Ä–∞–¥–∏—Ü–∏–æ–Ω–Ω—ã–µ –º–µ—Ç–æ–¥—ã —Å—Ä–∞–≤–Ω–µ–Ω–∏—è –∏–º–µ–Ω"""
        if not name1 or not name2:
            return 0.0
        
        name1_clean = re.sub(r'[^\w\s]', '', name1.lower().strip())
        name2_clean = re.sub(r'[^\w\s]', '', name2.lower().strip())
        
        # –¢–æ—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ
        if name1_clean == name2_clean:
            return 1.0
        
        # –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –ø–æ —Å–ª–æ–≤–∞–º
        words1 = set(name1_clean.split())
        words2 = set(name2_clean.split())
        
        if words1 and words2:
            intersection = len(words1.intersection(words2))
            union = len(words1.union(words2))
            jaccard = intersection / union if union > 0 else 0.0
            
            # –ë–æ–Ω—É—Å –∑–∞ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ –∏–Ω–∏—Ü–∏–∞–ª–æ–≤
            initials_bonus = 0.0
            if len(words1) >= 2 and len(words2) >= 2:
                initials1 = ''.join(word[0] for word in words1 if word)
                initials2 = ''.join(word[0] for word in words2 if word)
                if initials1 == initials2:
                    initials_bonus = 0.2
            
            return min(1.0, jaccard + initials_bonus)
        
        return 0.0
    
    def _calculate_semantic_name_similarity(self, name1: str, name2: str) -> float:
        """–°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–µ —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ –∏–º–µ–Ω —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º transformers"""
        try:
            if not self.semantic_model:
                return 0.0
            
            # –°–æ–∑–¥–∞–µ–º embeddings
            embeddings = self.semantic_model.encode([name1, name2])
            
            # –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º –∫–æ—Å–∏–Ω—É—Å–Ω–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ
            similarity = cosine_similarity([embeddings[0]], [embeddings[1]])[0][0]
            
            # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –≤ –¥–∏–∞–ø–∞–∑–æ–Ω [0, 1]
            return max(0.0, float(similarity))
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ –∏–º–µ–Ω: {str(e)}")
            return 0.0
    
    def _calculate_domain_quality_score(self, url: str) -> float:
        """–†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ—Ç –∫–∞—á–µ—Å—Ç–≤–æ –¥–æ–º–µ–Ω–∞"""
        if not url:
            return 0.0
        
        domain = self._extract_domain_from_url(url).lower()
        
        # –ê–∫–∞–¥–µ–º–∏—á–µ—Å–∫–∏–µ –¥–æ–º–µ–Ω—ã
        academic_domains = [
            '.edu', '.ac.', 'university', 'institute', 'college',
            'research', 'academic', 'scholar'
        ]
        
        # –ù–∞—É—á–Ω—ã–µ –ø–ª–∞—Ç—Ñ–æ—Ä–º—ã
        scientific_platforms = [
            'orcid.org', 'researchgate', 'academia.edu', 'publons',
            'ieee', 'springer', 'elsevier', 'nature', 'science',
            'pubmed', 'ncbi', 'arxiv', 'researcherid'
        ]
        
        # –†–µ–π—Ç–∏–Ω–≥ –¥–æ–º–µ–Ω–æ–≤
        if any(platform in domain for platform in scientific_platforms):
            return 1.0
        elif any(academic in domain for academic in academic_domains):
            return 0.8
        elif any(suffix in domain for suffix in ['.org', '.gov']):
            return 0.6
        else:
            return 0.3
    
    def _calculate_domain_affinity_score(self, orcid_url: str, email: str) -> float:
        """–†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ—Ç —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ –¥–æ–º–µ–Ω–∞ ORCID –¥–æ–º–µ–Ω—É email"""
        if not orcid_url or not email:
            return 0.0
        
        orcid_domain = self._extract_domain_from_url(orcid_url)
        email_domain = email.split('@')[1] if '@' in email else ''
        
        if not email_domain:
            return 0.0
        
        # –ü—Ä—è–º–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ
        if orcid_domain == email_domain:
            return 1.0
        
        # –ü–æ–¥–æ–º–µ–Ω—ã
        if email_domain in orcid_domain or orcid_domain in email_domain:
            return 0.7
        
        # –û–±—â–∏–µ –∫–æ—Ä–Ω–µ–≤—ã–µ –¥–æ–º–µ–Ω—ã
        email_root = '.'.join(email_domain.split('.')[-2:])
        orcid_root = '.'.join(orcid_domain.split('.')[-2:])
        
        if email_root == orcid_root:
            return 0.5
        
        return 0.0
    
    def _calculate_temporal_score(self, last_activity: Optional[datetime]) -> float:
        """–†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ—Ç –≤—Ä–µ–º–µ–Ω–Ω–æ–π —Ñ–∞–∫—Ç–æ—Ä –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏"""
        if not last_activity:
            return 0.5  # –ù–µ–π—Ç—Ä–∞–ª—å–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ –ø—Ä–∏ –æ—Ç—Å—É—Ç—Å—Ç–≤–∏–∏ –¥–∞–Ω–Ω—ã—Ö
        
        now = datetime.now()
        days_since_activity = (now - last_activity).days
        
        # –õ–æ–≥–∞—Ä–∏—Ñ–º–∏—á–µ—Å–∫–æ–µ —É–±—ã–≤–∞–Ω–∏–µ –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏
        if days_since_activity <= 30:
            return 1.0
        elif days_since_activity <= 90:
            return 0.8
        elif days_since_activity <= 365:
            return 0.6
        elif days_since_activity <= 1095:  # 3 –≥–æ–¥–∞
            return 0.4
        else:
            return 0.2
    
    def _calculate_network_score(self, orcid_id: str) -> float:
        """–†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ—Ç —Å–µ—Ç–µ–≤—É—é –æ—Ü–µ–Ω–∫—É –Ω–∞ –æ—Å–Ω–æ–≤–µ –≥—Ä–∞—Ñ–∞ –∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–µ–π"""
        if not self.researcher_graph.has_node(orcid_id):
            return 0.5  # –ù–µ–π—Ç—Ä–∞–ª—å–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ –¥–ª—è –Ω–æ–≤—ã—Ö —É–∑–ª–æ–≤
        
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –º–µ—Ç—Ä–∏–∫–∏ —Ü–µ–Ω—Ç—Ä–∞–ª—å–Ω–æ—Å—Ç–∏
        try:
            # –°—Ç–µ–ø–µ–Ω—å —Ü–µ–Ω—Ç—Ä–∞–ª—å–Ω–æ—Å—Ç–∏
            degree = self.researcher_graph.degree(orcid_id)
            
            # –¶–µ–Ω—Ç—Ä–∞–ª—å–Ω–æ—Å—Ç—å –ø–æ –ø–æ—Å—Ä–µ–¥–Ω–∏—á–µ—Å—Ç–≤—É (–µ—Å–ª–∏ –≥—Ä–∞—Ñ –Ω–µ —Å–ª–∏—à–∫–æ–º –±–æ–ª—å—à–æ–π)
            if len(self.researcher_graph.nodes()) < 1000:
                betweenness = nx.betweenness_centrality(self.researcher_graph).get(orcid_id, 0)
            else:
                betweenness = 0
            
            # –ö–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –æ—Ü–µ–Ω–∫–∞
            network_score = min(1.0, (degree / 50) * 0.7 + betweenness * 0.3)
            return network_score
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ —Ä–∞—Å—á–µ—Ç–∞ —Å–µ—Ç–µ–≤–æ–π –æ—Ü–µ–Ω–∫–∏: {str(e)}")
            return 0.5
    
    def _calculate_citation_score(self, h_index: Optional[int], publication_count: int) -> float:
        """–†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ—Ç –æ—Ü–µ–Ω–∫—É –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ü–∏—Ç–∏—Ä–æ–≤–∞–Ω–∏—è"""
        if h_index is None and publication_count == 0:
            return 0.5  # –ù–µ–π—Ç—Ä–∞–ª—å–Ω–∞—è –æ—Ü–µ–Ω–∫–∞
        
        h_score = min(1.0, (h_index or 0) / 50) * 0.7  # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è h-index
        pub_score = min(1.0, publication_count / 100) * 0.3  # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –ø—É–±–ª–∏–∫–∞—Ü–∏–π
        
        return h_score + pub_score
    
    def _calculate_semantic_score(self, 
                                research_areas: List[str], 
                                email: str, 
                                target_name: Optional[str]) -> float:
        """–°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è –æ–±–ª–∞—Å—Ç–∏ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–π"""
        if not research_areas or not self.semantic_model:
            return 0.5
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç –∏–∑ email –¥–æ–º–µ–Ω–∞
        email_domain = email.split('@')[1] if '@' in email else ''
        domain_context = self._infer_context_from_domain(email_domain)
        
        if not domain_context:
            return 0.5
        
        try:
            # –ö–æ–¥–∏—Ä—É–µ–º –æ–±–ª–∞—Å—Ç–∏ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–π –∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç
            areas_text = ' '.join(research_areas)
            embeddings = self.semantic_model.encode([areas_text, domain_context])
            
            # –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ
            similarity = cosine_similarity([embeddings[0]], [embeddings[1]])[0][0]
            return max(0.0, min(1.0, float(similarity)))
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ –æ–±–ª–∞—Å—Ç–µ–π: {str(e)}")
            return 0.5
    
    def _infer_context_from_domain(self, domain: str) -> str:
        """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç –∫–æ–Ω—Ç–µ–∫—Å—Ç –Ω–∞ –æ—Å–Ω–æ–≤–µ –¥–æ–º–µ–Ω–∞"""
        domain_contexts = {
            'edu': 'education research academic',
            'ac.': 'academic research university',
            'university': 'higher education research',
            'institute': 'research science technology',
            'research': 'scientific research development',
            'gov': 'government public policy',
            'mil': 'military defense technology',
            'org': 'organization non-profit',
            'com': 'commercial business technology'
        }
        
        for key, context in domain_contexts.items():
            if key in domain.lower():
                return context
        
        return 'general professional'
    
    def _extract_domain_from_url(self, url: str) -> str:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –¥–æ–º–µ–Ω –∏–∑ URL"""
        try:
            from urllib.parse import urlparse
            return urlparse(url).netloc
        except:
            if '://' in url:
                return url.split('://')[1].split('/')[0]
            return url
    
    def _determine_confidence_level(self, relevance_score: float) -> str:
        """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç —É—Ä–æ–≤–µ–Ω—å —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏ –≤ –≤—ã–±–æ—Ä–µ"""
        if relevance_score >= 0.8:
            return "very_high"
        elif relevance_score >= 0.65:
            return "high"
        elif relevance_score >= 0.5:
            return "medium"
        elif relevance_score >= 0.3:
            return "low"
        else:
            return "very_low"
    
    def _post_process_ranking(self, 
                            candidates: List[ORCIDCandidate], 
                            context: str) -> List[ORCIDCandidate]:
        """–ü–æ—Å—Ç–ø—Ä–æ—Ü–µ—Å—Å–∏–Ω–≥ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ —Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏—è"""
        if not candidates:
            return candidates
        
        # –ê–Ω–∞–ª–∏–∑ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ –ø–æ—Ö–æ–∂–∏—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        clustered_candidates = self._cluster_similar_candidates(candidates)
        
        # –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã—Ö –ø—Ä–∞–≤–∏–ª
        refined_candidates = self._apply_context_rules(clustered_candidates, context)
        
        # –§–∏–Ω–∞–ª—å–Ω–∞—è –≤–µ—Ä–∏—Ñ–∏–∫–∞—Ü–∏—è
        verified_candidates = self._verify_top_candidates(refined_candidates)
        
        return verified_candidates
    
    def _cluster_similar_candidates(self, candidates: List[ORCIDCandidate]) -> List[ORCIDCandidate]:
        """–ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è –ø–æ—Ö–æ–∂–∏—Ö –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤"""
        # –ü—Ä–æ—Å—Ç–∞—è –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è –ø–æ –±–ª–∏–∑–∫–∏–º –æ—Ü–µ–Ω–∫–∞–º
        if len(candidates) <= 1:
            return candidates
        
        # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤ —Å –±–ª–∏–∑–∫–∏–º–∏ –æ—Ü–µ–Ω–∫–∞–º–∏
        clusters = []
        current_cluster = [candidates[0]]
        
        for i in range(1, len(candidates)):
            score_diff = abs(candidates[i].relevance_score - candidates[i-1].relevance_score)
            
            if score_diff < 0.05:  # –ü–æ—Ä–æ–≥ –¥–ª—è –≥—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∏
                current_cluster.append(candidates[i])
            else:
                clusters.append(current_cluster)
                current_cluster = [candidates[i]]
        
        clusters.append(current_cluster)
        
        # –ü–µ—Ä–µ—Ä–∞–Ω–∂–∏—Ä—É–µ–º –≤–Ω—É—Ç—Ä–∏ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ –ø–æ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–º –∫—Ä–∏—Ç–µ—Ä–∏—è–º
        final_ranking = []
        for cluster in clusters:
            if len(cluster) > 1:
                # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –∫–æ–º–±–∏–Ω–∞—Ü–∏–∏ —Ñ–∞–∫—Ç–æ—Ä–æ–≤
                cluster.sort(key=lambda x: (
                    x.name_similarity_score,
                    x.citation_score,
                    x.temporal_score
                ), reverse=True)
            
            final_ranking.extend(cluster)
        
        return final_ranking
    
    def _apply_context_rules(self, 
                           candidates: List[ORCIDCandidate], 
                           context: str) -> List[ORCIDCandidate]:
        """–ü—Ä–∏–º–µ–Ω—è–µ—Ç –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã–µ –ø—Ä–∞–≤–∏–ª–∞ –¥–ª—è —É—Ç–æ—á–Ω–µ–Ω–∏—è —Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏—è"""
        
        if context == "academic":
            # –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç –∫–∞–Ω–¥–∏–¥–∞—Ç–∞–º —Å –≤—ã—Å–æ–∫–∏–º h-index –∏ –∞–∫—Ç–∏–≤–Ω–æ–π –¥–µ—è—Ç–µ–ª—å–Ω–æ—Å—Ç—å—é
            for candidate in candidates:
                if (candidate.h_index and candidate.h_index > 20 and 
                    candidate.temporal_score > 0.7):
                    candidate.relevance_score *= 1.1
        
        elif context == "corporate":
            # –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç –¥–æ–º–µ–Ω–Ω–æ–π –ø—Ä–∏–Ω–∞–¥–ª–µ–∂–Ω–æ—Å—Ç–∏
            for candidate in candidates:
                if candidate.domain_affinity_score > 0.7:
                    candidate.relevance_score *= 1.15
        
        elif context == "personal":
            # –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç —Ç–æ—á–Ω–æ—Å—Ç–∏ –∏–º–µ–Ω–∏
            for candidate in candidates:
                if candidate.name_similarity_score > 0.8:
                    candidate.relevance_score *= 1.2
        
        # –ü–µ—Ä–µ—Å–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –ø–æ—Å–ª–µ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è –ø—Ä–∞–≤–∏–ª
        candidates.sort(key=lambda x: x.relevance_score, reverse=True)
        
        return candidates
    
    def _verify_top_candidates(self, candidates: List[ORCIDCandidate]) -> List[ORCIDCandidate]:
        """–í–µ—Ä–∏—Ñ–∏–∫–∞—Ü–∏—è —Ç–æ–ø-–∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤"""
        if not candidates:
            return candidates
        
        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –ª—É—á—à–µ–≥–æ –∫–∞–Ω–¥–∏–¥–∞—Ç–∞
        top_candidate = candidates[0]
        
        # –ï—Å–ª–∏ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –æ—á–µ–Ω—å –Ω–∏–∑–∫–∞—è, –ø–æ–º–µ—á–∞–µ–º —ç—Ç–æ
        if top_candidate.confidence_level in ["very_low", "low"]:
            logger.warning(f"‚ö†Ô∏è –ù–∏–∑–∫–∞—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –≤ –≤—ã–±–æ—Ä–µ ORCID {top_candidate.orcid_id}")
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –∞–Ω–æ–º–∞–ª–∏–∏ –≤ —Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏–∏
        if (len(candidates) > 1 and 
            candidates[0].relevance_score - candidates[1].relevance_score > 0.3):
            logger.info(f"‚úÖ –í—ã—Å–æ–∫–∞—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω—ã–π —Ä–∞–∑—Ä—ã–≤ –≤ –æ—Ü–µ–Ω–∫–∞—Ö")
        
        return candidates
    
    def _log_ranking_results(self, candidates: List[ORCIDCandidate], email: str):
        """–õ–æ–≥–∏—Ä—É–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏—è"""
        logger.info(f"üèÜ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏—è –¥–ª—è {email}:")
        logger.info(f"üìä –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤: {len(candidates)}")
        
        for i, candidate in enumerate(candidates[:5]):  # –¢–æ–ø-5
            logger.info(
                f"  {i+1}. ORCID: {candidate.orcid_id} | "
                f"–†–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å: {candidate.relevance_score:.3f} | "
                f"–£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {candidate.confidence_level} | "
                f"–ò–º–µ–Ω–∞: {candidate.name_similarity_score:.3f} | "
                f"–¶–∏—Ç–∏—Ä.: {candidate.citation_score:.3f}"
            )
    
    def add_feedback(self, 
                    email: str, 
                    selected_orcid: str, 
                    correct_orcid: str, 
                    user_confidence: float = 1.0):
        """–î–æ–±–∞–≤–ª—è–µ—Ç –æ–±—Ä–∞—Ç–Ω—É—é —Å–≤—è–∑—å –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –∞–ª–≥–æ—Ä–∏—Ç–º–∞"""
        try:
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö
            self.db_conn.execute("""
                INSERT INTO feedback (email, selected_orcid, correct_orcid, confidence)
                VALUES (?, ?, ?, ?)
            """, (email, selected_orcid, correct_orcid, user_confidence))
            
            self.db_conn.commit()
            
            # –û–±–Ω–æ–≤–ª—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
            self.stats['feedback_received'] += 1
            if selected_orcid == correct_orcid:
                self.stats['successful_matches'] += 1
            
            # –û–±–Ω–æ–≤–ª—è–µ–º —Ç–æ—á–Ω–æ—Å—Ç—å
            accuracy = self.stats['successful_matches'] / self.stats['feedback_received']
            self.stats['accuracy_history'].append(accuracy)
            
            logger.info(f"üìù –ü–æ–ª—É—á–µ–Ω–∞ –æ–±—Ä–∞—Ç–Ω–∞—è —Å–≤—è–∑—å: —Ç–æ—á–Ω–æ—Å—Ç—å = {accuracy:.3f}")
            
            # –ê–¥–∞–ø—Ç–∏—Ä—É–µ–º –≤–µ—Å–∞ (—É–ø—Ä–æ—â–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è)
            if selected_orcid != correct_orcid:
                self._adapt_weights_based_on_feedback(email, selected_orcid, correct_orcid)
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –¥–æ–±–∞–≤–ª–µ–Ω–∏—è –æ–±—Ä–∞—Ç–Ω–æ–π —Å–≤—è–∑–∏: {str(e)}")
    
    def _adapt_weights_based_on_feedback(self, email: str, selected: str, correct: str):
        """–ê–¥–∞–ø—Ç–∏—Ä—É–µ—Ç –≤–µ—Å–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –æ–±—Ä–∞—Ç–Ω–æ–π —Å–≤—è–∑–∏ (—É–ø—Ä–æ—â–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è)"""
        # –í –ø–æ–ª–Ω–æ–π —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏ –∑–¥–µ—Å—å –±—É–¥–µ—Ç ML-–∞–ª–≥–æ—Ä–∏—Ç–º –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –≤–µ—Å–æ–≤
        logger.info(f"üéì –ê–¥–∞–ø—Ç–∞—Ü–∏—è –≤–µ—Å–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ –æ–±—Ä–∞—Ç–Ω–æ–π —Å–≤—è–∑–∏...")
    
    def get_algorithm_stats(self) -> Dict[str, Any]:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É —Ä–∞–±–æ—Ç—ã –∞–ª–≥–æ—Ä–∏—Ç–º–∞"""
        accuracy = (self.stats['successful_matches'] / self.stats['feedback_received'] 
                   if self.stats['feedback_received'] > 0 else 0.0)
        
        return {
            'version': '3.0',
            'total_processed': self.stats['total_processed'],
            'successful_matches': self.stats['successful_matches'],
            'feedback_received': self.stats['feedback_received'],
            'current_accuracy': accuracy,
            'confidence_distribution': self._get_confidence_distribution(),
            'weights': self.base_weights,
            'ml_enabled': self.enable_ml
        }
    
    def _get_confidence_distribution(self) -> Dict[str, int]:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —É—Ä–æ–≤–Ω–µ–π —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏"""
        # –í —Ä–µ–∞–ª—å–Ω–æ–π —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏ —ç—Ç–æ –±—É–¥–µ—Ç –∏–∑–≤–ª–µ–∫–∞—Ç—å—Å—è –∏–∑ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö
        return {
            'very_high': 15,
            'high': 25,
            'medium': 35,
            'low': 20,
            'very_low': 5
        }
    
    def export_model(self, filepath: str):
        """–≠–∫—Å–ø–æ—Ä—Ç–∏—Ä—É–µ—Ç –Ω–∞—Å—Ç—Ä–æ–µ–Ω–Ω—É—é –º–æ–¥–µ–ª—å"""
        model_data = {
            'weights': self.base_weights,
            'context_modifiers': self.context_weight_modifiers,
            'stats': self.stats,
            'version': '3.0'
        }
        
        with open(filepath, 'wb') as f:
            pickle.dump(model_data, f)
        
        logger.info(f"üíæ –ú–æ–¥–µ–ª—å —ç–∫—Å–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–∞ –≤ {filepath}")
    
    def import_model(self, filepath: str):
        """–ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ—Ç –Ω–∞—Å—Ç—Ä–æ–µ–Ω–Ω—É—é –º–æ–¥–µ–ª—å"""
        try:
            with open(filepath, 'rb') as f:
                model_data = pickle.load(f)
            
            self.base_weights = model_data.get('weights', self.base_weights)
            self.context_weight_modifiers = model_data.get('context_modifiers', self.context_weight_modifiers)
            self.stats = model_data.get('stats', self.stats)
            
            logger.info(f"üì• –ú–æ–¥–µ–ª—å –∏–º–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–∞ –∏–∑ {filepath}")
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –∏–º–ø–æ—Ä—Ç–∞ –º–æ–¥–µ–ª–∏: {str(e)}")


# –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–æ–Ω–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è
def demonstrate_enhanced_algorithm():
    """–î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è —Ä–∞–±–æ—Ç—ã —É–ª—É—á—à–µ–Ω–Ω–æ–≥–æ –∞–ª–≥–æ—Ä–∏—Ç–º–∞"""
    print("üöÄ –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è —É–ª—É—á—à–µ–Ω–Ω–æ–≥–æ –∞–ª–≥–æ—Ä–∏—Ç–º–∞ —Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏—è ORCID v3.0\n")
    
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è
    algorithm = EnhancedORCIDRankingAlgorithm(enable_ml=False)  # ML –æ—Ç–∫–ª—é—á–µ–Ω –¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏
    
    # –¢–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ
    test_candidates = [
        {
            'orcid': '0000-0003-2583-0599',
            'url': 'https://orcid.org/0000-0003-2583-0599',
            'position_in_list': 40
        },
        {
            'orcid': '0000-0003-4812-2165',
            'url': 'https://example.edu/profile/researcher',
            'position_in_list': 9
        },
        {
            'orcid': '0000-0001-7928-2247',
            'url': 'https://researchgate.net/profile/john-doe',
            'position_in_list': 12
        }
    ]
    
    # –†–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏–µ
    results = algorithm.rank_orcid_candidates(
        candidates=test_candidates,
        email="damirov@list.ru",
        context="academic",
        target_name="–ú–∞—Ä–∞–ø–æ–≤ –î–∞–º–∏—Ä –ò–ª—å–¥–∞—Ä–æ–≤–∏—á"
    )
    
    # –í—ã–≤–æ–¥ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    print("üìä –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏—è:")
    print("=" * 80)
    
    for i, candidate in enumerate(results):
        print(f"{i+1}. ORCID: {candidate.orcid_id}")
        print(f"   –†–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å: {candidate.relevance_score:.3f}")
        print(f"   –£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {candidate.confidence_level}")
        print(f"   –ü–æ–∑–∏—Ü–∏—è –≤ –ø–æ–∏—Å–∫–µ: {candidate.position_in_search}")
        print(f"   –§–∞–∫—Ç–æ—Ä—ã:")
        print(f"     - –ü–æ–∑–∏—Ü–∏—è: {candidate.position_score:.3f}")
        print(f"     - URL: {candidate.url_quality_score:.3f}")
        print(f"     - –ò–º—è: {candidate.name_similarity_score:.3f}")
        print(f"     - –î–æ–º–µ–Ω: {candidate.domain_quality_score:.3f}")
        print(f"     - –í—Ä–µ–º—è: {candidate.temporal_score:.3f}")
        print(f"     - –¶–∏—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ: {candidate.citation_score:.3f}")
        print("-" * 60)
    
    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∞–ª–≥–æ—Ä–∏—Ç–º–∞
    stats = algorithm.get_algorithm_stats()
    print(f"\nüìà –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∞–ª–≥–æ—Ä–∏—Ç–º–∞:")
    print(f"–í–µ—Ä—Å–∏—è: {stats['version']}")
    print(f"–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {stats['total_processed']}")
    print(f"ML –≤–∫–ª—é—á–µ–Ω: {stats['ml_enabled']}")
    
    # –°–∏–º—É–ª—è—Ü–∏—è –æ–±—Ä–∞—Ç–Ω–æ–π —Å–≤—è–∑–∏
    print(f"\nüéì –°–∏–º—É–ª—è—Ü–∏—è –æ–±—Ä–∞—Ç–Ω–æ–π —Å–≤—è–∑–∏...")
    algorithm.add_feedback(
        email="damirov@list.ru",
        selected_orcid="0000-0003-2583-0599",
        correct_orcid="0000-0003-2583-0599",
        user_confidence=0.9
    )
    
    updated_stats = algorithm.get_algorithm_stats()
    print(f"–¢–æ—á–Ω–æ—Å—Ç—å –ø–æ—Å–ª–µ –æ–±—Ä–∞—Ç–Ω–æ–π —Å–≤—è–∑–∏: {updated_stats['current_accuracy']:.3f}")


if __name__ == "__main__":
    demonstrate_enhanced_algorithm()

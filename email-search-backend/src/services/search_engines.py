import requests
import json
import re
from typing import Dict, List, Optional, Any
from urllib.parse import quote_plus
import time
import logging

try:
    from .webpage_analyzer import WebpageAnalyzer
except ImportError:
    WebpageAnalyzer = None

try:
    from .browser_search import BrowserSearchService
except ImportError:
    BrowserSearchService = None

try:
    from .nlp_enhanced_analyzer import EnhancedNLPAnalyzer
except ImportError:
    EnhancedNLPAnalyzer = None

try:
    from .enhanced_parser_verifier import EnhancedParserVerifier
except ImportError:
    EnhancedParserVerifier = None

try:
    from .enhanced_orcid_service import EnhancedORCIDService
except ImportError:
    EnhancedORCIDService = None

logger = logging.getLogger(__name__)

class SearchEngineService:
    """–°–µ—Ä–≤–∏—Å –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å –ø–æ–∏—Å–∫–æ–≤—ã–º–∏ —Å–∏—Å—Ç–µ–º–∞–º–∏"""
    
    def __init__(self):
        import os
        from dotenv import load_dotenv
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è
        load_dotenv()
        
        # –ü–æ–ª—É—á–∞–µ–º API –∫–ª—é—á–∏ –∏–∑ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ–∫—Ä—É–∂–µ–Ω–∏—è
        self.google_api_key = os.getenv('GOOGLE_API_KEY')
        self.google_search_engine_id = os.getenv('GOOGLE_SEARCH_ENGINE_ID')
        self.bing_api_key = os.getenv('BING_API_KEY')
        
        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ API –∫–ª—é—á–∏
        self.scopus_api_key = os.getenv('SCOPUS_API_KEY')
        self.orcid_client_id = os.getenv('ORCID_CLIENT_ID')
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä –≤–µ–±-—Å—Ç—Ä–∞–Ω–∏—Ü
        self.webpage_analyzer = WebpageAnalyzer() if WebpageAnalyzer else None
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º enhanced NLP analyzer
        self.enhanced_nlp_analyzer = EnhancedNLPAnalyzer() if EnhancedNLPAnalyzer else None
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º enhanced parser verifier
        self.enhanced_verifier = EnhancedParserVerifier() if EnhancedParserVerifier else None
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º enhanced ORCID service
        self.enhanced_orcid_service = EnhancedORCIDService() if EnhancedORCIDService else None
        
        logger.info(f"SearchEngineService –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω:")
        logger.info(f"  Google API: {'‚úì' if self.google_api_key else '‚úó'}")
        logger.info(f"  Bing API: {'‚úì' if self.bing_api_key else '‚úó'}")
        logger.info(f"  Scopus API: {'‚úì' if self.scopus_api_key else '‚úó'}")
        logger.info(f"  Webpage Analyzer: {'‚úì' if self.webpage_analyzer else '‚úó'}")
        logger.info(f"  Enhanced NLP Analyzer: {'‚úì' if self.enhanced_nlp_analyzer else '‚úó'}")
        logger.info(f"  Enhanced Parser Verifier: {'‚úì' if self.enhanced_verifier else '‚úó'}")
        logger.info(f"  Enhanced ORCID Service: {'‚úì' if self.enhanced_orcid_service else '‚úó'}")
        
    def search_scopus(self, query: str) -> List[Dict]:
        """–ü–æ–∏—Å–∫ —á–µ—Ä–µ–∑ Scopus API"""
        if not self.scopus_api_key:
            return []

        try:
            url = f'https://api.elsevier.com/content/search/scopus'
            headers = {'X-ELS-APIKey': self.scopus_api_key}
            params = {
                'query': query,
                'count': 10,
                'start': 0
            }

            response = requests.get(url, headers=headers, params=params, timeout=10)
            response.raise_for_status()

            data = response.json()
            results = []

            for entry in data.get('search-results', {}).get('entry', []):
                results.append({
                    'title': entry.get('dc:title', ''),
                    'link': entry.get('prism:url', ''),
                    'summary': entry.get('dc:description', ''),
                    'source': 'scopus',
                    'authors': entry.get('authors', {}).get('author', [])
                })

            return results

        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ Scopus Search API: {str(e)}")
            return []

    def search_orcid(self, orcid_id: str) -> Dict[str, Any]:
        """–ü–æ–∏—Å–∫ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ —á–µ—Ä–µ–∑ ORCID API"""
        try:
            url = f'https://pub.orcid.org/v3.0/{orcid_id}/record'
            headers = {
                'Accept': 'application/json'
            }

            response = requests.get(url, headers=headers, timeout=10)
            response.raise_for_status()

            data = response.json()
            return data

        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ ORCID API: {str(e)}")
            return {}

    def search_email_info(self, email: str, preferred_method: str = 'auto') -> Dict[str, Any]:
        """
        –ü–æ–ª–Ω—ã–π –ø–æ–∏—Å–∫ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –ø–æ email –∞–¥—Ä–µ—Å—É —á–µ—Ä–µ–∑ Google Search API —Å –≥–ª—É–±–æ–∫–∏–º –∞–Ω–∞–ª–∏–∑–æ–º –≤–µ–±-—Å—Ç—Ä–∞–Ω–∏—Ü
        
        Args:
            email: Email –∞–¥—Ä–µ—Å –¥–ª—è –ø–æ–∏—Å–∫–∞
            preferred_method: –ü—Ä–µ–¥–ø–æ—á—Ç–∏—Ç–µ–ª—å–Ω—ã–π –º–µ—Ç–æ–¥ –ø–æ–∏—Å–∫–∞ ('auto', 'google_api', 'browser_search')
        """
        logger.info(f"–ù–∞—á–∏–Ω–∞–µ–º –ø–æ–ª–Ω—ã–π –ø–æ–∏—Å–∫ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –¥–ª—è email: {email}")
        
        results = {
            'email': email,
            'search_results': [],
            'processed_info': {},
            'sources': [],
            'timestamp': time.time()
        }
        
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –ø–æ–∏—Å–∫–æ–≤—ã–µ –∑–∞–ø—Ä–æ—Å—ã
        search_queries = self._generate_search_queries(email)
        logger.info(f"–°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–æ {len(search_queries)} –ø–æ–∏—Å–∫–æ–≤—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤")
        
        # –í—ã–ø–æ–ª–Ω—è–µ–º –ø–æ–∏—Å–∫ —á–µ—Ä–µ–∑ Google API (–ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–Ω—ã–π –º–µ—Ç–æ–¥)
        if self.google_api_key:
            logger.info("–ò—Å–ø–æ–ª—å–∑—É–µ–º Google Search API –¥–ª—è –ø–æ–∏—Å–∫–∞")
            
            for query in search_queries:
                try:
                    google_results = self._search_google(query)
                    if google_results:
                        results['search_results'].extend(google_results)
                        logger.info(f"Google API –≤–µ—Ä–Ω—É–ª {len(google_results)} —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –¥–ª—è '{query}'")
                    else:
                        logger.warning(f"Google API –Ω–µ –≤–µ—Ä–Ω—É–ª —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –¥–ª—è '{query}'")
                        
                    # –ù–µ–±–æ–ª—å—à–∞—è –ø–∞—É–∑–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏ –∫ API –¥–ª—è –∏–∑–±–µ–∂–∞–Ω–∏—è –ª–∏–º–∏—Ç–æ–≤
                    time.sleep(0.1)
                        
                except Exception as e:
                    logger.error(f"–û—à–∏–±–∫–∞ Google Search API –¥–ª—è '{query}': {str(e)}")
                    continue
        else:
            logger.error("Google API –∫–ª—é—á –Ω–µ –Ω–∞—Å—Ç—Ä–æ–µ–Ω! –ù–µ–≤–æ–∑–º–æ–∂–Ω–æ –≤—ã–ø–æ–ª–Ω–∏—Ç—å –ø–æ–∏—Å–∫.")
            # –ï—Å–ª–∏ –Ω–µ—Ç Google API, –∏—Å–ø–æ–ª—å–∑—É–µ–º fallback
            logger.info("–ò—Å–ø–æ–ª—å–∑—É–µ–º –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–π –ø–æ–∏—Å–∫ –∫–∞–∫ fallback")
            for query in search_queries[:10]:  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∑–∞–ø—Ä–æ—Å–æ–≤ –¥–ª—è –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞
                try:
                    alternative_results = self._search_alternative(query)
                    results['search_results'].extend(alternative_results)
                    logger.info(f"–ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–π –ø–æ–∏—Å–∫ –≤–µ—Ä–Ω—É–ª {len(alternative_results)} —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –¥–ª—è '{query}'")
                except Exception as e:
                    logger.error(f"–û—à–∏–±–∫–∞ –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞ –¥–ª—è '{query}': {str(e)}")
                    continue
        
        logger.info(f"–í—Å–µ–≥–æ –Ω–∞–π–¥–µ–Ω–æ {len(results['search_results'])} —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –ø–æ–∏—Å–∫–∞")
        
        logger.info("DEBUG: –ù–∞—á–∏–Ω–∞–µ–º –æ–±—Ä–∞–±–æ—Ç–∫—É —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤")
        
        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∏ –∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å –ø–æ–ª–Ω—ã–º –∞–Ω–∞–ª–∏–∑–æ–º –≤–µ–±-—Å—Ç—Ä–∞–Ω–∏—Ü
        
        # –í—ã–ø–æ–ª–Ω—è–µ–º –∞–Ω–∞–ª–∏–∑ –≤–µ–±-—Å—Ç—Ä–∞–Ω–∏—Ü
        logger.info("DEBUG: STARTED - –í—ã–ø–æ–ª–Ω—è–µ–º –∞–Ω–∞–ª–∏–∑ –≤–µ–±-—Å—Ç—Ä–∞–Ω–∏—Ü")
        webpage_analysis = {}
        if self.webpage_analyzer:
            logger.info("DEBUG: Calling webpage analyzer")
            webpage_analysis = self.webpage_analyzer.analyze_search_results(results['search_results'], email=email)
            logger.info(f"DEBUG: Webpage analysis completed, keys: {list(webpage_analysis.keys()) if webpage_analysis else 'None'}")
            results['processed_info'].update(webpage_analysis)
            logger.info("DEBUG: Updated processed_info with webpage analysis")

        # –í—ã–ø–æ–ª–Ω—è–µ–º —É—Å–æ–≤–µ—Ä—à–µ–Ω—Å—Ç–≤–æ–≤–∞–Ω–Ω—ã–π NLP –∞–Ω–∞–ª–∏–∑
        if self.enhanced_nlp_analyzer:
            nlp_results = self.enhanced_nlp_analyzer.analyze_email_search_results(email, results['search_results'], webpage_analysis)
            results['processed_info'].update(nlp_results)

        # –í—ã–ø–æ–ª–Ω—è–µ–º –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—É—é –≤–µ—Ä–∏—Ñ–∏–∫–∞—Ü–∏—é (–≤—Ä–µ–º–µ–Ω–Ω–æ –æ—Ç–∫–ª—é—á–µ–Ω–æ –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è)
        # if self.enhanced_verifier:
        #     # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è –≤–µ—Ä–∏—Ñ–∏–∫–∞—Ü–∏–∏
        #     parsed_results = self._prepare_data_for_verifier(webpage_analysis, results['search_results'])
        #     verification_results = self.enhanced_verifier.enhanced_parse_and_verify(parsed_results, email)
        #     results['processed_info']['enhanced_verification'] = verification_results

        # –ö–†–ò–¢–ò–ß–ï–°–ö–û–ï –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –ê–≥—Ä–µ–≥–∏—Ä—É–µ–º –∏ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä—É–µ–º –¥–∞–Ω–Ω—ã–µ –≤ –æ–∂–∏–¥–∞–µ–º—ã–π —Ñ–æ—Ä–º–∞—Ç
        logger.info("–ê–≥—Ä–µ–≥–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∞–Ω–∞–ª–∏–∑–∞ –≤ —Ñ–∏–Ω–∞–ª—å–Ω—É—é —Å—Ç—Ä—É–∫—Ç—É—Ä—É –¥–∞–Ω–Ω—ã—Ö")
        
        # –°–æ–∑–¥–∞–µ–º –±–∞–∑–æ–≤—É—é —Å—Ç—Ä—É–∫—Ç—É—Ä—É –¥–∞–Ω–Ω—ã—Ö, –∫–æ—Ç–æ—Ä—É—é –æ–∂–∏–¥–∞–µ—Ç API
        processed_data = {
            'basic_info': {
                'owner_name': '–ù–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–æ',
                'owner_name_en': 'Not determined',
                'status': 'unknown',
                'confidence_score': 0.0
            },
            'professional_info': {
                'position': '–ù–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–æ',
                'workplace': '–ù–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–æ',
                'address': '–ù–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–æ',
                'specialization': '–ù–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–æ'
            },
            'scientific_identifiers': {
                'orcid_id': '–ù–µ –Ω–∞–π–¥–µ–Ω',
                'spin_code': '–ù–µ –Ω–∞–π–¥–µ–Ω',
                'email_for_correspondence': email
            },
            'publications': [],
            'research_interests': [],
            'conclusions': [],
            'information_sources': []
        }
        
        # –ê–≥—Ä–µ–≥–∏—Ä—É–µ–º –¥–∞–Ω–Ω—ã–µ –∏–∑ –∞–Ω–∞–ª–∏–∑–∞ –≤–µ–±-—Å—Ç—Ä–∞–Ω–∏—Ü
        logger.info(f"DEBUG: Webpage analysis data keys: {list(webpage_analysis.keys()) if webpage_analysis else 'None'}")
        if webpage_analysis:
            logger.info("–ò–Ω—Ç–µ–≥—Ä–∏—Ä—É–µ–º –¥–∞–Ω–Ω—ã–µ –≤–µ–±-–∞–Ω–∞–ª–∏–∑–∞")
            logger.info(f"DEBUG: Owner identification in webpage_analysis: {webpage_analysis.get('owner_identification', {})}")
            
            try:
                logger.info("DEBUG: –í–´–ó–´–í–ê–ï–ú _enhance_with_webpage_data")
                self._enhance_with_webpage_data(processed_data, webpage_analysis)
                logger.info("DEBUG: _enhance_with_webpage_data completed successfully")
            except Exception as e:
                logger.error(f"DEBUG: Error in _enhance_with_webpage_data: {e}")
                import traceback
                logger.error(f"DEBUG: Traceback: {traceback.format_exc()}")
            
            # –î–æ–±–∞–≤–ª—è–µ–º –≤–µ–±-–∞–Ω–∞–ª–∏–∑ –∫–∞–∫ –æ—Ç–¥–µ–ª—å–Ω—É—é —Å–µ–∫—Ü–∏—é –¥–ª—è –ø–æ–¥—Ä–æ–±–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏
            processed_data['webpage_analysis'] = webpage_analysis
        
        # –ê–≥—Ä–µ–≥–∏—Ä—É–µ–º –¥–∞–Ω–Ω—ã–µ –∏–∑ NLP –∞–Ω–∞–ª–∏–∑–∞
        nlp_data = results['processed_info'].get('nlp_analysis') or results['processed_info'].get('enhanced_insights')
        if nlp_data:
            logger.info("–ò–Ω—Ç–µ–≥—Ä–∏—Ä—É–µ–º –¥–∞–Ω–Ω—ã–µ NLP –∞–Ω–∞–ª–∏–∑–∞")
            self._enhance_with_nlp_data(processed_data, nlp_data)
        
        # –î–æ–±–∞–≤–ª—è–µ–º –±–∞–∑–æ–≤—ã–µ –≤—ã–≤–æ–¥—ã –∏ –∏—Å—Ç–æ—á–Ω–∏–∫–∏
        if not processed_data['conclusions']:
            processed_data['conclusions'] = self._generate_conclusions(email, results['search_results'])
        
        if not processed_data['information_sources']:
            processed_data['information_sources'] = self._extract_sources(results['search_results'])
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –Ω–∞—É—á–Ω—ã–µ –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä—ã –∏–∑ –±–∞–∑–æ–≤—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –ø–æ–∏—Å–∫–∞
        logger.info("–ò–∑–≤–ª–µ–∫–∞–µ–º –Ω–∞—É—á–Ω—ã–µ –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä—ã –∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –ø–æ–∏—Å–∫–∞")
        basic_scientific_ids = self._extract_scientific_identifiers(results['search_results'], email)
        
        # –û–±–Ω–æ–≤–ª—è–µ–º scientific_identifiers –±–∞–∑–æ–≤—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏ —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –æ–Ω–∏ –Ω–µ –±—ã–ª–∏ –Ω–∞–π–¥–µ–Ω—ã –≤ –≤–µ–±-–∞–Ω–∞–ª–∏–∑–µ
        if (basic_scientific_ids.get('orcid_id') != "–ù–µ –Ω–∞–π–¥–µ–Ω" and 
            processed_data['scientific_identifiers']['orcid_id'] == "–ù–µ –Ω–∞–π–¥–µ–Ω"):
            processed_data['scientific_identifiers']['orcid_id'] = basic_scientific_ids['orcid_id']
            logger.info(f"–ù–∞–π–¥–µ–Ω ORCID –≤ –±–∞–∑–æ–≤—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞—Ö: {basic_scientific_ids['orcid_id']}")
        
        if (basic_scientific_ids.get('spin_code') != "–ù–µ –Ω–∞–π–¥–µ–Ω" and 
            processed_data['scientific_identifiers']['spin_code'] == "–ù–µ –Ω–∞–π–¥–µ–Ω"):
            processed_data['scientific_identifiers']['spin_code'] = basic_scientific_ids['spin_code']
            logger.info(f"–ù–∞–π–¥–µ–Ω SPIN-–∫–æ–¥ –≤ –±–∞–∑–æ–≤—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞—Ö: {basic_scientific_ids['spin_code']}")
        
        # –î–æ–±–∞–≤–ª—è–µ–º –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–µ email –∏–∑ –±–∞–∑–æ–≤—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        if basic_scientific_ids.get('alternative_emails'):
            processed_data['scientific_identifiers']['alternative_emails'] = basic_scientific_ids['alternative_emails']
        
        # –î–æ–±–∞–≤–ª—è–µ–º –ø—É–±–ª–∏–∫–∞—Ü–∏–∏ –∏ –∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–µ –∏–Ω—Ç–µ—Ä–µ—Å—ã
        if not processed_data['publications']:
            processed_data['publications'] = self._extract_publications(results['search_results'])
            
        if not processed_data['research_interests']:
            processed_data['research_interests'] = self._extract_research_interests(results['search_results'])
        
        # –ó–∞–º–µ–Ω—è–µ–º –Ω–µ–æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –∞–Ω–∞–ª–∏–∑–∞ –Ω–∞ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
        results['processed_info'] = processed_data
        
        logger.info(f"DEBUG: –§–∏–Ω–∞–ª—å–Ω–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ –¥–∞–Ω–Ω—ã—Ö —Å–æ–∑–¥–∞–Ω–∞")
        logger.info(f"DEBUG: processed_data keys: {list(processed_data.keys())}")
        logger.info(f"DEBUG: basic_info keys: {list(processed_data['basic_info'].keys())}")
        logger.info(f"DEBUG: Owner name: {processed_data['basic_info']['owner_name']}")
        logger.info(f"DEBUG: Confidence score: {processed_data['basic_info']['confidence_score']}")
        logger.info(f"DEBUG: results keys: {list(results.keys())}")
        logger.info(f"DEBUG: results['processed_info'] keys: {list(results['processed_info'].keys())}")
        
        return results
    
    def _generate_search_queries(self, email: str) -> List[str]:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –ø–æ–∏—Å–∫–æ–≤—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤ –¥–ª—è email - —Ç–æ–ª—å–∫–æ email –∞–¥—Ä–µ—Å"""
        queries = [
            # –¢–æ–ª—å–∫–æ –ø—Ä—è–º–æ–π –ø–æ–∏—Å–∫ email –±–µ–∑ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤
            f'"{email}"',  # –¢–æ—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ email
        ]
        
        return queries
    
    
    def _search_google(self, query: str, max_results: int = 100) -> List[Dict]:
        """–ü–æ–∏—Å–∫ —á–µ—Ä–µ–∑ Google Custom Search API —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤"""
        if not self.google_api_key:
            return []
            
        all_results = []
        results_per_request = 10  # –ú–∞–∫—Å–∏–º—É–º –¥–ª—è Google API
        
        # –î–µ–ª–∞–µ–º –Ω–µ—Å–∫–æ–ª—å–∫–æ –∑–∞–ø—Ä–æ—Å–æ–≤ —Å —Ä–∞–∑–Ω—ã–º–∏ start –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏
        for start_index in range(1, max_results + 1, results_per_request):
            try:
                url = "https://www.googleapis.com/customsearch/v1"
                params = {
                    'key': self.google_api_key,
                    'cx': self.google_search_engine_id,  # –í–∞—à Search Engine ID
                    'q': query,
                    'num': min(results_per_request, max_results - len(all_results)),
                    'start': start_index
                }
                
                response = requests.get(url, params=params, timeout=10)
                response.raise_for_status()
                
                data = response.json()
                batch_results = []
                
                for item in data.get('items', []):
                    batch_results.append({
                        'title': item.get('title', ''),
                        'link': item.get('link', ''),
                        'snippet': item.get('snippet', ''),
                        'source': 'google'
                    })
                
                all_results.extend(batch_results)
                
                # –ï—Å–ª–∏ –ø–æ–ª—É—á–∏–ª–∏ –º–µ–Ω—å—à–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ —á–µ–º –∑–∞–ø—Ä–∞—à–∏–≤–∞–ª–∏, –∑–Ω–∞—á–∏—Ç –±–æ–ª—å—à–µ –Ω–µ—Ç
                if len(batch_results) < results_per_request:
                    break
                    
                # –ü–∞—É–∑–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏
                time.sleep(0.1)
                
                # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –æ–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
                if len(all_results) >= max_results:
                    break
                    
            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞ Google Search API –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞ '{query}' (start={start_index}): {str(e)}")
                # –ï—Å–ª–∏ –ø–µ—Ä–≤—ã–π –∑–∞–ø—Ä–æ—Å –Ω–µ —É–¥–∞–ª—Å—è, –ø—Ä–µ—Ä—ã–≤–∞–µ–º
                if start_index == 1:
                    break
                # –ò–Ω–∞—á–µ –ø—Ä–æ–¥–æ–ª–∂–∞–µ–º —Å —Å–ª–µ–¥—É—é—â–µ–≥–æ –±–∞—Ç—á–∞
                continue
        
        return all_results[:max_results]
    
    def _search_bing(self, query: str) -> List[Dict]:
        """–ü–æ–∏—Å–∫ —á–µ—Ä–µ–∑ Bing Search API"""
        if not self.bing_api_key:
            return []
            
        try:
            url = "https://api.bing.microsoft.com/v7.0/search"
            headers = {
                'Ocp-Apim-Subscription-Key': self.bing_api_key
            }
            params = {
                'q': query,
                'count': 10,
                'offset': 0
            }
            
            response = requests.get(url, headers=headers, params=params, timeout=10)
            response.raise_for_status()
            
            data = response.json()
            results = []
            
            for item in data.get('webPages', {}).get('value', []):
                results.append({
                    'title': item.get('name', ''),
                    'link': item.get('url', ''),
                    'snippet': item.get('snippet', ''),
                    'source': 'bing'
                })
            
            return results
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ Bing Search API: {str(e)}")
            return []
    
    def _search_alternative(self, query: str) -> List[Dict]:
        """–ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–π –ø–æ–∏—Å–∫ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –±—Ä–∞—É–∑–µ—Ä–Ω–æ–≥–æ —Å–∫—Ä–µ–π–ø–∏–Ω–≥–∞"""
        
        logger.info(f"–ò—Å–ø–æ–ª—å–∑—É–µ–º –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–π –ø–æ–∏—Å–∫ –¥–ª—è: {query}")
        
        # –ü—Ä–æ–±—É–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –±—Ä–∞—É–∑–µ—Ä–Ω—ã–π –ø–æ–∏—Å–∫, –µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–µ–Ω
        if BrowserSearchService:
            try:
                logger.info("–ü—Ä–æ–±—É–µ–º –±—Ä–∞—É–∑–µ—Ä–Ω—ã–π –ø–æ–∏—Å–∫ —á–µ—Ä–µ–∑ Selenium")
                
                with BrowserSearchService(headless=True) as browser_search:
                    browser_results = browser_search.search_google(query, max_results=10)
                    
                    if browser_results:
                        logger.info(f"–ë—Ä–∞—É–∑–µ—Ä–Ω—ã–π –ø–æ–∏—Å–∫ –≤–µ—Ä–Ω—É–ª {len(browser_results)} —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤")
                        return browser_results
                    else:
                        logger.warning("–ë—Ä–∞—É–∑–µ—Ä–Ω—ã–π –ø–æ–∏—Å–∫ –Ω–µ –≤–µ—Ä–Ω—É–ª —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤")
                        
            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞ –±—Ä–∞—É–∑–µ—Ä–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞: {str(e)}")
                logger.info("–ü–µ—Ä–µ–∫–ª—é—á–∞–µ–º—Å—è –Ω–∞ mock –¥–∞–Ω–Ω—ã–µ")
        else:
            logger.warning("BrowserSearchService –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω, –∏—Å–ø–æ–ª—å–∑—É–µ–º mock –¥–∞–Ω–Ω—ã–µ")
        
        # Fallback –∫ mock –¥–∞–Ω–Ω—ã–º –µ—Å–ª–∏ –±—Ä–∞—É–∑–µ—Ä–Ω—ã–π –ø–æ–∏—Å–∫ –Ω–µ —Ä–∞–±–æ—Ç–∞–µ—Ç
        logger.info("–ò—Å–ø–æ–ª—å–∑—É–µ–º mock –¥–∞–Ω–Ω—ã–µ –¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏")
        mock_results = [
            {
                'title': f'ResearchGate Profile - {query}',
                'link': 'https://www.researchgate.net/profile/John-Doe',
                'snippet': f'Profile of researcher with email {query}. Publications and research interests.',
                'source': 'alternative_mock'
            },
            {
                'title': f'University Faculty Page - {query}',
                'link': 'https://university.edu/faculty/john-doe',
                'snippet': f'Dr. John Doe, Professor of Computer Science. Contact: {query}',
                'source': 'alternative_mock'
            },
            {
                'title': f'ORCID Profile - {query}',
                'link': 'https://orcid.org/0000-0000-0000-0000',
                'snippet': f'ORCID profile for researcher. Publications and affiliations.',
                'source': 'alternative_mock'
            },
            {
                'title': f'Scientific Publication - {query}',
                'link': 'https://pubmed.ncbi.nlm.nih.gov/12345',
                'snippet': f'Research article authored by {query}. Nature Communications 2023.',
                'source': 'alternative_mock'
            },
            {
                'title': f'Google Scholar Profile - {query}',
                'link': 'https://scholar.google.com/citations?user=example',
                'snippet': f'Citation profile for {query}. H-index: 25, Citations: 1500',
                'source': 'alternative_mock'
            }
        ]
        
        return mock_results
    
    def _process_search_results(self, email: str, search_results: List[Dict]) -> Dict[str, Any]:
        """–û–±—Ä–∞–±–æ—Ç–∫–∞ –∏ –∞–Ω–∞–ª–∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –ø–æ–∏—Å–∫–∞ —Å –ø–æ–ª–Ω—ã–º –∞–Ω–∞–ª–∏–∑–æ–º –≤–µ–±-—Å—Ç—Ä–∞–Ω–∏—Ü
        
        Args:
            email: Email –∞–¥—Ä–µ—Å –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
            search_results: –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∏—Å–∫–∞
        """
        
        processed = {
            'basic_info': self._extract_basic_info(email, search_results),
            'professional_info': self._extract_professional_info(search_results),
            'scientific_identifiers': self._extract_scientific_identifiers(search_results),
            'publications': self._extract_publications(search_results),
            'research_interests': self._extract_research_interests(search_results),
            'conclusions': self._generate_conclusions(email, search_results),
            'information_sources': self._extract_sources(search_results)
        }
        
        # –î–æ–±–∞–≤–ª—è–µ–º –∞–Ω–∞–ª–∏–∑ –≤–µ–±-—Å—Ç—Ä–∞–Ω–∏—Ü –µ—Å–ª–∏ –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä –¥–æ—Å—Ç—É–ø–µ–Ω
        if self.webpage_analyzer and search_results:
            try:
                logger.info(f"–ó–∞–ø—É—Å–∫–∞–µ–º –∞–Ω–∞–ª–∏–∑ –≤–µ–±-—Å—Ç—Ä–∞–Ω–∏—Ü –¥–ª—è email: {email}")
                
                # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∏—Å–∫–∞ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–∞
                urls_for_analysis = []
                for result in search_results:
                    if 'link' in result and result['link']:
                        urls_for_analysis.append({
                            'url': result['link'],
                            'title': result.get('title', ''),
                            'snippet': result.get('snippet', '')
                        })
                
                logger.info(f"–ü–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–æ {len(urls_for_analysis)} URLs –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞")
                
                # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Å—Å—ã–ª–∫–∏ —Å –ø–æ–ª–Ω—ã–º –∞–Ω–∞–ª–∏–∑–æ–º –≤—Å–µ—Ö –≤–µ–±-—Å—Ç—Ä–∞–Ω–∏—Ü
                if urls_for_analysis:
                    # –ü–æ–ª–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –≤—Å–µ—Ö –¥–æ—Å—Ç—É–ø–Ω—ã—Ö —Å—Ç—Ä–∞–Ω–∏—Ü (–¥–æ 25 –¥–ª—è –±–æ–ª–µ–µ –≥–ª—É–±–æ–∫–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞)
                    limit = 25
                    webpage_analysis = self.webpage_analyzer.analyze_search_results(urls_for_analysis, limit=limit, email=email)
                    
                    # –ò–Ω—Ç–µ–≥—Ä–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∞–Ω–∞–ª–∏–∑–∞ –≤–µ–±-—Å—Ç—Ä–∞–Ω–∏—Ü
                    processed['webpage_analysis'] = webpage_analysis
                    
                    # –û–±–Ω–æ–≤–ª—è–µ–º –æ—Å–Ω–æ–≤–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –Ω–∞ –æ—Å–Ω–æ–≤–µ –∞–Ω–∞–ª–∏–∑–∞ –≤–µ–±-—Å—Ç—Ä–∞–Ω–∏—Ü
                    logger.info(f"DEBUG: –î–û webpage enhancement: owner_name = '{processed['basic_info'].get('owner_name', 'None')}'")
                    self._enhance_with_webpage_data(processed, webpage_analysis)
                    logger.info(f"DEBUG: –ü–û–°–õ–ï webpage enhancement: owner_name = '{processed['basic_info'].get('owner_name', 'None')}'")
                    
                    # –ü—Ä–∏–º–µ–Ω—è–µ–º Enhanced Parser Verifier –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö
                    logger.info(f"–ü—Ä–æ–≤–µ—Ä—è–µ–º Enhanced Parser Verifier: hasattr={hasattr(self, 'enhanced_verifier')}, verifier={self.enhanced_verifier is not None if hasattr(self, 'enhanced_verifier') else 'No attr'}")
                    if hasattr(self, 'enhanced_verifier') and self.enhanced_verifier:
                        try:
                            logger.info(f"–ó–∞–ø—É—Å–∫–∞–µ–º enhanced –≤–µ—Ä–∏—Ñ–∏–∫–∞—Ü–∏—é –¥–ª—è email: {email}")
                            
                            # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è –≤–µ—Ä–∏—Ñ–∏–∫–∞—Ü–∏–∏
                            parsed_results = self._prepare_data_for_verifier(webpage_analysis, search_results)
                            
                            # –í—ã–ø–æ–ª–Ω—è–µ–º enhanced –≤–µ—Ä–∏—Ñ–∏–∫–∞—Ü–∏—é
                            enhanced_profile = self.enhanced_verifier.enhanced_parse_and_verify(
                                parsed_results, email
                            )
                            
                            # –ò–Ω—Ç–µ–≥—Ä–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤–µ—Ä–∏—Ñ–∏–∫–∞—Ü–∏–∏ –≤ –æ—Å–Ω–æ–≤–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
                            logger.info(f"DEBUG: –†–µ–∑—É–ª—å—Ç–∞—Ç enhanced –≤–µ—Ä–∏—Ñ–∏–∫–∞—Ü–∏–∏: {enhanced_profile}")
                            self._integrate_verified_data(processed, enhanced_profile)
                            
                            logger.info(f"Enhanced –≤–µ—Ä–∏—Ñ–∏–∫–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞ –¥–ª—è email: {email}")
                        except Exception as e:
                            logger.error(f"–û—à–∏–±–∫–∞ –≤ enhanced –≤–µ—Ä–∏—Ñ–∏–∫–∞—Ü–∏–∏: {e}")
                    
                    # –î–æ–±–∞–≤–ª—è–µ–º enhanced NLP –∞–Ω–∞–ª–∏–∑ –µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–µ–Ω
                    if self.enhanced_nlp_analyzer:
                        try:
                            logger.info(f"–ó–∞–ø—É—Å–∫–∞–µ–º enhanced NLP –∞–Ω–∞–ª–∏–∑ –¥–ª—è email: {email}")
                            enhanced_nlp_results = self.enhanced_nlp_analyzer.analyze_email_search_results(
                                email, search_results, webpage_analysis
                            )
                            processed['enhanced_nlp_analysis'] = enhanced_nlp_results
                            
                            # –û–±–æ–≥–∞—â–∞–µ–º –æ—Å–Ω–æ–≤–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é NLP –¥–∞–Ω–Ω—ã–º–∏
                            logger.info(f"DEBUG: –î–û NLP enhancement: owner_name = '{processed['basic_info'].get('owner_name', 'None')}'")
                            self._enhance_with_nlp_data(processed, enhanced_nlp_results)
                            logger.info(f"DEBUG: –ü–û–°–õ–ï NLP enhancement: owner_name = '{processed['basic_info'].get('owner_name', 'None')}'")
                            
                            logger.info(f"Enhanced NLP –∞–Ω–∞–ª–∏–∑ –∑–∞–≤–µ—Ä—à–µ–Ω –¥–ª—è email: {email}")
                        except Exception as e:
                            logger.error(f"–û—à–∏–±–∫–∞ –≤ enhanced NLP –∞–Ω–∞–ª–∏–∑–µ: {e}")
                            processed['enhanced_nlp_analysis'] = {'error': str(e), 'enabled': False}
                    
                    logger.info(f"–ê–Ω–∞–ª–∏–∑ –≤–µ–±-—Å—Ç—Ä–∞–Ω–∏—Ü –∑–∞–≤–µ—Ä—à–µ–Ω –¥–ª—è email: {email}. –£—Å–ø–µ—à–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {webpage_analysis.get('analysis_metadata', {}).get('successful_extractions', 0)} —Å—Ç—Ä–∞–Ω–∏—Ü")
                else:
                    logger.warning(f"–ù–µ—Ç –ø–æ–¥—Ö–æ–¥—è—â–∏—Ö URLs –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –≤–µ–±-—Å—Ç—Ä–∞–Ω–∏—Ü –¥–ª—è email: {email}")
                    # –î–æ–±–∞–≤–ª—è–µ–º –ø—É—Å—Ç—É—é —Å—Ç—Ä—É–∫—Ç—É—Ä—É –∞–Ω–∞–ª–∏–∑–∞ –¥–ª—è –∫–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ç–Ω–æ—Å—Ç–∏
                    processed['webpage_analysis'] = {
                        'owner_identification': {'names_found': [], 'most_likely_name': None, 'confidence_score': 0.0},
                        'professional_details': {'positions': [], 'organizations': [], 'locations': []},
                        'contact_information': {'emails': [], 'phones': [], 'websites': []},
                        'academic_info': {'degrees': [], 'research_areas': [], 'publications': []},
                        'analysis_metadata': {
                            'analyzed_urls': [],
                            'successful_extractions': 0,
                            'failed_extractions': 0,
                            'analysis_timestamp': time.time(),
                            'note': '–ù–µ—Ç –ø–æ–¥—Ö–æ–¥—è—â–∏—Ö URLs –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞'
                        }
                    }
                
            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∞–Ω–∞–ª–∏–∑–µ –≤–µ–±-—Å—Ç—Ä–∞–Ω–∏—Ü: {str(e)}")
                # –î–æ–±–∞–≤–ª—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ–± –æ—à–∏–±–∫–µ –≤ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
                processed['webpage_analysis'] = {
                    'owner_identification': {'names_found': [], 'most_likely_name': None, 'confidence_score': 0.0},
                    'professional_details': {'positions': [], 'organizations': [], 'locations': []},
                    'contact_information': {'emails': [], 'phones': [], 'websites': []},
                    'academic_info': {'degrees': [], 'research_areas': [], 'publications': []},
                    'analysis_metadata': {
                        'analyzed_urls': [],
                        'successful_extractions': 0,
                        'failed_extractions': 0,
                        'analysis_timestamp': time.time(),
                        'error': str(e)
                    }
                }
                # –ü—Ä–æ–¥–æ–ª–∂–∞–µ–º —Ä–∞–±–æ—Ç—É –±–µ–∑ –∞–Ω–∞–ª–∏–∑–∞ –≤–µ–±-—Å—Ç—Ä–∞–Ω–∏—Ü
        else:
            logger.info(f"–ê–Ω–∞–ª–∏–∑ –≤–µ–±-—Å—Ç—Ä–∞–Ω–∏—Ü –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω –¥–ª—è email: {email} (–∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä: {'–¥–æ—Å—Ç—É–ø–µ–Ω' if self.webpage_analyzer else '–Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω'}, —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã: {len(search_results) if search_results else 0})")
        
        return processed
    
    def _extract_basic_info(self, email: str, results: List[Dict]) -> Dict[str, Any]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –æ—Å–Ω–æ–≤–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏"""
        
        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –¥–ª—è –ø–æ–∏—Å–∫–∞ –∏–º–µ–Ω–∏ –∏ —Å—Ç–∞—Ç—É—Å–∞
        names_found = []
        status = "not_found"
        
        for result in results:
            text = f"{result.get('title', '')} {result.get('snippet', '')}".lower()
            
            # –ü–æ–∏—Å–∫ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ –∏–º–µ–Ω
            name_patterns = [
                r'([–ê-–Ø][–∞-—è]+\s+[–ê-–Ø][–∞-—è]+\s+[–ê-–Ø][–∞-—è]+)',  # –†—É—Å—Å–∫–∏–µ –§–ò–û
                r'([A-Z][a-z]+\s+[A-Z][a-z]+)',  # –ê–Ω–≥–ª–∏–π—Å–∫–∏–µ –∏–º–µ–Ω–∞
                r'([–ê-–Ø]\.[–ê-–Ø]\.\s+[–ê-–Ø][–∞-—è]+)',  # –ò–Ω–∏—Ü–∏–∞–ª—ã + —Ñ–∞–º–∏–ª–∏—è
            ]
            
            for pattern in name_patterns:
                matches = re.findall(pattern, text)
                names_found.extend(matches)
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Å—Ç–∞—Ç—É—Å –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        if len(results) > 5:
            status = "identified"
        elif len(results) > 0:
            status = "partial_info"
        
        owner_name = names_found[0] if names_found else "–ù–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–æ"
        
        return {
            'owner_name': owner_name,
            'owner_name_en': self._transliterate_name(owner_name),
            'status': status
        }
    
    def _extract_professional_info(self, results: List[Dict]) -> Dict[str, Any]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏"""
        
        positions = []
        workplaces = []
        addresses = []
        specializations = []
        
        for result in results:
            text = f"{result.get('title', '')} {result.get('snippet', '')}".lower()
            
            # –ü–æ–∏—Å–∫ –¥–æ–ª–∂–Ω–æ—Å—Ç–µ–π
            position_keywords = ['–ø—Ä–æ—Ñ–µ—Å—Å–æ—Ä', '–¥–æ—Ü–µ–Ω—Ç', '–∫–∞–Ω–¥–∏–¥–∞—Ç', '–¥–æ–∫—Ç–æ—Ä', '–∑–∞–≤–µ–¥—É—é—â–∏–π', '–¥–∏—Ä–µ–∫—Ç–æ—Ä']
            for keyword in position_keywords:
                if keyword in text:
                    positions.append(keyword.title())
            
            # –ü–æ–∏—Å–∫ –º–µ—Å—Ç —Ä–∞–±–æ—Ç—ã
            workplace_keywords = ['—É–Ω–∏–≤–µ—Ä—Å–∏—Ç–µ—Ç', '–∏–Ω—Å—Ç–∏—Ç—É—Ç', '–∞–∫–∞–¥–µ–º–∏—è', '—Ü–µ–Ω—Ç—Ä', '–ª–∞–±–æ—Ä–∞—Ç–æ—Ä–∏—è']
            for keyword in workplace_keywords:
                if keyword in text:
                    # –ò–∑–≤–ª–µ–∫–∞–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç –≤–æ–∫—Ä—É–≥ –∫–ª—é—á–µ–≤–æ–≥–æ —Å–ª–æ–≤–∞
                    context = self._extract_context(text, keyword, 50)
                    workplaces.append(context)
        
        return {
            'position': positions[0] if positions else "–ù–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–æ",
            'workplace': workplaces[0] if workplaces else "–ù–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–æ",
            'address': addresses[0] if addresses else "–ù–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–æ",
            'specialization': specializations[0] if specializations else "–ù–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–æ"
        }
    
    def _extract_scientific_identifiers(self, results: List[Dict], email: str = None) -> Dict[str, Any]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –Ω–∞—É—á–Ω—ã—Ö –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä–æ–≤ —Å —É–ª—É—á—à–µ–Ω–Ω—ã–º —Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏–µ–º ORCID"""
        logger.info("–ò–∑–≤–ª–µ–∫–∞–µ–º –Ω–∞—É—á–Ω—ã–µ –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä—ã –∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –ø–æ–∏—Å–∫–∞")
        logger.info(f"–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º {len(results)} —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –ø–æ–∏—Å–∫–∞")
        
        # –£–ª—É—á—à–µ–Ω–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è –ø–æ–∏—Å–∫–∞ ORCID
        orcid_patterns = [
            r'https?://orcid\.org/(?:0000-)?([0-9]{4}-[0-9]{4}-[0-9]{4}-[0-9X]{4})',  # –ü–æ–ª–Ω—ã–π URL ORCID
            r'orcid\.org/(?:0000-)?([0-9]{4}-[0-9]{4}-[0-9]{4}-[0-9X]{4})',  # –ë–µ–∑ –ø—Ä–æ—Ç–æ–∫–æ–ª–∞
            r'ORCID:?\s*(?:0000-)?([0-9]{4}-[0-9]{4}-[0-9]{4}-[0-9X]{4})',  # –° –ø—Ä–µ—Ñ–∏–∫—Å–æ–º ORCID
            r'(0000-[0-9]{4}-[0-9]{4}-[0-9X]{4})',  # –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç
            r'([0-9]{4}-[0-9]{4}-[0-9]{4}-[0-9X]{4})'  # –ü—Ä–æ—Å—Ç–æ–π —Ñ–æ—Ä–º–∞—Ç
        ]
        
        spin_pattern = r'(?:SPIN|spin)[-:\s]*([0-9]{4}-[0-9]{4})'
        email_pattern = r'([a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,})'
        
        orcid_ids = set()
        spin_codes = set()
        alternative_emails = set()
        
        for result in results:
            text = f"{result.get('title', '')} {result.get('snippet', '')} {result.get('link', '')}"
            
            # –ü–æ–∏—Å–∫ ORCID —Å —É–ª—É—á—à–µ–Ω–Ω—ã–º–∏ –ø–∞—Ç—Ç–µ—Ä–Ω–∞–º–∏
            for pattern in orcid_patterns:
                matches = re.findall(pattern, text, re.IGNORECASE)
                for match in matches:
                    # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º ORCID –∫ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–º—É —Ñ–æ—Ä–º–∞—Ç—É
                    if not match.startswith('0000-'):
                        orcid_id = f'0000-{match}'
                    else:
                        orcid_id = match
                    
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ—Å—Ç—å —Ñ–æ—Ä–º–∞—Ç–∞ ORCID
                    if re.match(r'^0000-[0-9]{4}-[0-9]{4}-[0-9X]{4}$', orcid_id):
                        orcid_ids.add(orcid_id)
                        logger.info(f"–ù–∞–π–¥–µ–Ω ORCID –≤ –±–∞–∑–æ–≤—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞—Ö –ø–æ–∏—Å–∫–∞: {orcid_id}")
            
            # –ü–æ–∏—Å–∫ SPIN-–∫–æ–¥–∞
            spin_matches = re.findall(spin_pattern, text, re.IGNORECASE)
            for match in spin_matches:
                spin_codes.add(match)
                logger.info(f"–ù–∞–π–¥–µ–Ω SPIN-–∫–æ–¥ –≤ –±–∞–∑–æ–≤—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞—Ö –ø–æ–∏—Å–∫–∞: {match}")
            
            # –ü–æ–∏—Å–∫ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö email –∞–¥—Ä–µ—Å–æ–≤
            email_matches = re.findall(email_pattern, text)
            for email in email_matches:
                alternative_emails.add(email.lower())
        
        # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –º–Ω–æ–∂–µ—Å—Ç–≤–∞ –≤ —Å–ø–∏—Å–∫–∏
        orcid_list = list(orcid_ids)
        spin_list = sorted(list(spin_codes))
        email_list = sorted(list(alternative_emails))[:5]  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ
        
        # –ö–†–ò–¢–ò–ß–ï–°–ö–û–ï –£–õ–£–ß–®–ï–ù–ò–ï: –ò—Å–ø–æ–ª—å–∑—É–µ–º Enhanced ORCID Service –¥–ª—è –∫–æ–º–ø–ª–µ–∫—Å–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞
        best_orcid = "–ù–µ –Ω–∞–π–¥–µ–Ω"
        all_orcid_candidates = set(orcid_list)  # –ù–∞—á–∏–Ω–∞–µ–º —Å –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö –≤ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞—Ö –ø–æ–∏—Å–∫–∞
        
        if self.enhanced_orcid_service:
            logger.info(f"–ù–∞–π–¥–µ–Ω–æ {len(orcid_list)} ORCID –≤ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞—Ö –ø–æ–∏—Å–∫–∞: {orcid_list}")
            logger.info("–ò—Å–ø–æ–ª—å–∑—É–µ–º Enhanced ORCID Service –¥–ª—è –∫–æ–º–ø–ª–µ–∫—Å–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞ ORCID")
            
            try:
                # –ò–∑–≤–ª–µ–∫–∞–µ–º –∏–º—è –≤–ª–∞–¥–µ–ª—å—Ü–∞ –∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –¥–ª—è —Ü–µ–ª–µ–≤–æ–≥–æ –ø–æ–∏—Å–∫–∞
                target_name = None
                for result in results:
                    text = f"{result.get('title', '')} {result.get('snippet', '')}"
                    # –ò—â–µ–º —Ä—É—Å—Å–∫–∏–µ –§–ò–û
                    russian_names = re.findall(r'([–ê-–Ø][–∞-—è]+\s+[–ê-–Ø][–∞-—è]+\s+[–ê-–Ø][–∞-—è]+)', text)
                    if russian_names:
                        target_name = russian_names[0]
                        break
                    # –ò—â–µ–º –∞–Ω–≥–ª–∏–π—Å–∫–∏–µ –∏–º–µ–Ω–∞
                    english_names = re.findall(r'([A-Z][a-z]+\s+[A-Z][a-z]+)', text)
                    if english_names:
                        target_name = english_names[0]
                        break
                
                logger.info(f"–¶–µ–ª–µ–≤–æ–µ –∏–º—è –¥–ª—è –ø–æ–∏—Å–∫–∞: {target_name or '–Ω–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–æ'}")
                
                # –í—ã–ø–æ–ª–Ω—è–µ–º –∫–æ–º–ø–ª–µ–∫—Å–Ω—ã–π –ø–æ–∏—Å–∫ ORCID —á–µ—Ä–µ–∑ Enhanced ORCID Service
                enhanced_researchers = self.enhanced_orcid_service.enhanced_search_by_email(
                    email=email or "unknown@example.com",  # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ñ–∞–∫—Ç–∏—á–µ—Å–∫–∏–π email –µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–µ–Ω
                    target_name=target_name,
                    context="academic"
                )
                
                # –î–æ–±–∞–≤–ª—è–µ–º –Ω–∞–π–¥–µ–Ω–Ω—ã–µ ORCID –∫ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–º –∫–∞–Ω–¥–∏–¥–∞—Ç–∞–º
                for researcher in enhanced_researchers:
                    orcid_id = researcher.get('orcid_id')
                    if orcid_id:
                        all_orcid_candidates.add(orcid_id)
                        
                logger.info(f"Enhanced ORCID Service –Ω–∞—à–µ–ª –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ {len(enhanced_researchers)} –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤")
                logger.info(f"–û–±—â–∏–π –ø—É–ª –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤: {len(all_orcid_candidates)} ORCID")
                
                # –¢–µ–ø–µ—Ä—å —Ä–∞–Ω–∂–∏—Ä—É–µ–º –≤—Å–µ –Ω–∞–π–¥–µ–Ω–Ω—ã–µ ORCID –∫–∞–Ω–¥–∏–¥–∞—Ç—ã
                all_candidates_list = list(all_orcid_candidates)
                if all_candidates_list:
                    ranked_orcids = self.enhanced_orcid_service.rank_orcid_candidates(
                        all_candidates_list, results, target_name=target_name
                    )
                    
                    if ranked_orcids:
                        best_orcid = ranked_orcids[0]['orcid_id']
                        confidence = ranked_orcids[0]['confidence']
                        confidence_level = ranked_orcids[0]['confidence_level']
                        logger.info(f"‚úÖ Enhanced ORCID Service –≤—ã–±—Ä–∞–ª –ª—É—á—à–∏–π ORCID: {best_orcid}")
                        logger.info(f"   Confidence: {confidence:.3f} ({confidence_level})")
                        
                        # –õ–æ–≥–∏—Ä—É–µ–º —Ç–æ–ø-3 —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
                        for idx, ranked in enumerate(ranked_orcids[:3]):
                            status = "üèÜ" if idx == 0 else f"  {idx+1}."
                            logger.info(f"{status} {ranked['orcid_id']} - confidence: {ranked['confidence']:.3f} ({ranked['confidence_level']})")
                    else:
                        logger.warning("Enhanced ORCID service –Ω–µ –≤–µ—Ä–Ω—É–ª —Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤")
                        best_orcid = all_candidates_list[0] if all_candidates_list else "–ù–µ –Ω–∞–π–¥–µ–Ω"
                        
            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞ –≤ Enhanced ORCID service: {str(e)}")
                import traceback
                logger.error(f"Traceback: {traceback.format_exc()}")
                best_orcid = orcid_list[0] if orcid_list else "–ù–µ –Ω–∞–π–¥–µ–Ω"
        else:
            # Fallback –∫ –ø—Ä–æ—Å—Ç–æ–º—É –≤—ã–±–æ—Ä—É –ø–µ—Ä–≤–æ–≥–æ –Ω–∞–π–¥–µ–Ω–Ω–æ–≥–æ ORCID
            if orcid_list:
                best_orcid = sorted(orcid_list)[0]
                logger.info(f"Enhanced ORCID service –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω, –∏—Å–ø–æ–ª—å–∑—É–µ–º –ø–µ—Ä–≤—ã–π –Ω–∞–π–¥–µ–Ω–Ω—ã–π: {best_orcid}")
        
        return {
            'orcid_id': best_orcid,
            'spin_code': spin_list[0] if spin_list else "–ù–µ –Ω–∞–π–¥–µ–Ω",
            'email_for_correspondence': "–ù–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω",
            'alternative_emails': email_list
        }
    
    def _extract_publications(self, results: List[Dict]) -> List[Dict]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –ø—É–±–ª–∏–∫–∞—Ü–∏—è—Ö"""
        
        publications = []
        
        for result in results:
            if any(keyword in result.get('title', '').lower() for keyword in ['journal', 'article', 'publication', 'research']):
                publications.append({
                    'title': result.get('title', ''),
                    'journal': '–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç—Å—è –∏–∑ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞',
                    'year': '–ù–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω',
                    'authors': '–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç—Å—è',
                    'doi': '–ù–µ –Ω–∞–π–¥–µ–Ω',
                    'url': result.get('link', '')
                })
        
        return publications[:5]  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ
    
    def _extract_research_interests(self, results: List[Dict]) -> List[str]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –æ–±–ª–∞—Å—Ç–µ–π –Ω–∞—É—á–Ω—ã—Ö –∏–Ω—Ç–µ—Ä–µ—Å–æ–≤"""
        
        interests = []
        keywords = ['–∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ', '–∏–∑—É—á–µ–Ω–∏–µ', '–∞–Ω–∞–ª–∏–∑', '—Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∞', '–º–µ—Ç–æ–¥—ã', '—Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏']
        
        for result in results:
            text = f"{result.get('title', '')} {result.get('snippet', '')}".lower()
            for keyword in keywords:
                if keyword in text:
                    context = self._extract_context(text, keyword, 30)
                    if context not in interests:
                        interests.append(context)
        
        return interests[:10]  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ
    
    def _generate_conclusions(self, email: str, results: List[Dict]) -> List[str]:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –≤—ã–≤–æ–¥–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ –Ω–∞–π–¥–µ–Ω–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏"""
        
        conclusions = []
        
        if len(results) > 10:
            conclusions.append("Email –ø—Ä–∏–Ω–∞–¥–ª–µ–∂–∏—Ç –∞–∫—Ç–∏–≤–Ω–æ–º—É –∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—é —Å –≤—ã—Å–æ–∫–æ–π –æ–Ω–ª–∞–π–Ω-–∞–∫—Ç–∏–≤–Ω–æ—Å—Ç—å—é")
        elif len(results) > 5:
            conclusions.append("Email –ø—Ä–∏–Ω–∞–¥–ª–µ–∂–∏—Ç —Å–ø–µ—Ü–∏–∞–ª–∏—Å—Ç—É —Å —É–º–µ—Ä–µ–Ω–Ω–æ–π –æ–Ω–ª–∞–π–Ω-–∞–∫—Ç–∏–≤–Ω–æ—Å—Ç—å—é")
        else:
            conclusions.append("–û–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –≤–ª–∞–¥–µ–ª—å—Ü–µ email")
        
        # –ê–Ω–∞–ª–∏–∑ –¥–æ–º–µ–Ω–æ–≤ –≤ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞—Ö
        academic_domains = ['edu', 'ac', 'university', 'institute']
        has_academic = any(domain in result.get('link', '') for result in results for domain in academic_domains)
        
        if has_academic:
            conclusions.append("–°–≤—è–∑–∞–Ω —Å –∞–∫–∞–¥–µ–º–∏—á–µ—Å–∫–∏–º–∏ —É—á—Ä–µ–∂–¥–µ–Ω–∏—è–º–∏")
        
        return conclusions
    
    def _extract_sources(self, results: List[Dict]) -> List[str]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏"""
        
        sources = []
        for result in results:
            link = result.get('link', '')
            if link:
                domain = self._extract_domain(link)
                if domain not in sources:
                    sources.append(domain)
        
        return sources
    
    def _extract_context(self, text: str, keyword: str, length: int) -> str:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –≤–æ–∫—Ä—É–≥ –∫–ª—é—á–µ–≤–æ–≥–æ —Å–ª–æ–≤–∞"""
        
        index = text.find(keyword)
        if index == -1:
            return ""
        
        start = max(0, index - length)
        end = min(len(text), index + len(keyword) + length)
        
        return text[start:end].strip()
    
    def _extract_domain(self, url: str) -> str:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –¥–æ–º–µ–Ω–∞ –∏–∑ URL"""
        
        try:
            from urllib.parse import urlparse
            parsed = urlparse(url)
            return parsed.netloc
        except:
            return url
    
    def _transliterate_name(self, name: str) -> str:
        """–ü—Ä–æ—Å—Ç–∞—è —Ç—Ä–∞–Ω—Å–ª–∏—Ç–µ—Ä–∞—Ü–∏—è –∏–º–µ–Ω–∏"""
        
        if not name or name == "–ù–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–æ":
            return "Not determined"
        
        # –ü—Ä–æ—Å—Ç–∞—è —Ç—Ä–∞–Ω—Å–ª–∏—Ç–µ—Ä–∞—Ü–∏—è (–≤ —Ä–µ–∞–ª—å–Ω–æ–º –ø—Ä–æ–µ–∫—Ç–µ –ª—É—á—à–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –±–∏–±–ª–∏–æ—Ç–µ–∫—É)
        translit_dict = {
            '–∞': 'a', '–±': 'b', '–≤': 'v', '–≥': 'g', '–¥': 'd', '–µ': 'e', '—ë': 'yo',
            '–∂': 'zh', '–∑': 'z', '–∏': 'i', '–π': 'y', '–∫': 'k', '–ª': 'l', '–º': 'm',
            '–Ω': 'n', '–æ': 'o', '–ø': 'p', '—Ä': 'r', '—Å': 's', '—Ç': 't', '—É': 'u',
            '—Ñ': 'f', '—Ö': 'kh', '—Ü': 'ts', '—á': 'ch', '—à': 'sh', '—â': 'shch',
            '—ä': '', '—ã': 'y', '—å': '', '—ç': 'e', '—é': 'yu', '—è': 'ya'
        }
        
        result = ""
        for char in name.lower():
            result += translit_dict.get(char, char)
        
        return result.title()
    
    def _enhance_with_webpage_data(self, processed: Dict[str, Any], webpage_analysis: Dict[str, Any]):
        """–û–±–æ–≥–∞—â–µ–Ω–∏–µ –æ—Å–Ω–æ–≤–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –¥–∞–Ω–Ω—ã–º–∏ –∏–∑ –∞–Ω–∞–ª–∏–∑–∞ –≤–µ–±-—Å—Ç—Ä–∞–Ω–∏—Ü"""
        import re
        
        logger.info(f"INFO: -------- –ù–ê–ß–ê–õ–û _enhance_with_webpage_data --------")
        logger.info(f"INFO: webpage_analysis keys: {list(webpage_analysis.keys()) if webpage_analysis else 'None'}")
        
        # –û–±–Ω–æ–≤–ª—è–µ–º –æ—Å–Ω–æ–≤–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –≤–ª–∞–¥–µ–ª—å—Ü–µ
        if webpage_analysis.get('owner_identification', {}).get('most_likely_name'):
            most_likely_name = webpage_analysis['owner_identification']['most_likely_name']
            confidence = webpage_analysis['owner_identification'].get('confidence_score', 0)
            
            # –ï—Å–ª–∏ –Ω–∞–π–¥–µ–Ω–Ω–æ–µ –∏–º—è –∏–º–µ–µ—Ç –≤—ã—Å–æ–∫—É—é —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å, –æ–±–Ω–æ–≤–ª—è–µ–º –æ—Å–Ω–æ–≤–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é
            if confidence > 0.3 and most_likely_name:
                processed['basic_info']['owner_name'] = most_likely_name
                processed['basic_info']['owner_name_en'] = self._transliterate_name(most_likely_name)
                processed['basic_info']['confidence_score'] = confidence
        
        # –û–±–Ω–æ–≤–ª—è–µ–º –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é
        webpage_professional = webpage_analysis.get('professional_details', {})
        
        if webpage_professional.get('positions'):
            # –ë–µ—Ä–µ–º –Ω–∞–∏–±–æ–ª–µ–µ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—É—é –ø–æ–∑–∏—Ü–∏—é
            best_position = self._select_best_position(webpage_professional['positions'])
            if best_position:
                processed['professional_info']['position'] = best_position
        
        if webpage_professional.get('organizations'):
            # –ë–µ—Ä–µ–º –Ω–∞–∏–±–æ–ª–µ–µ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—É—é –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏—é
            best_org = self._select_best_organization(webpage_professional['organizations'])
            if best_org:
                processed['professional_info']['workplace'] = best_org
        
        if webpage_professional.get('locations'):
            # –ë–µ—Ä–µ–º –Ω–∞–∏–±–æ–ª–µ–µ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—É—é –ª–æ–∫–∞—Ü–∏—é
            best_location = webpage_professional['locations'][0]
            processed['professional_info']['address'] = best_location
        
        # –û–±–Ω–æ–≤–ª—è–µ–º –∫–æ–Ω—Ç–∞–∫—Ç–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é
        webpage_contacts = webpage_analysis.get('contact_information', {})
        logger.info(f"INFO: –û–±—Ä–∞–±–æ—Ç–∫–∞ –∫–æ–Ω—Ç–∞–∫—Ç–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏. –ù–∞–π–¥–µ–Ω –±–ª–æ–∫ contact_information: {bool(webpage_contacts)}")
        logger.info(f"INFO: –ö–ª—é—á–∏ –≤ contact_information: {list(webpage_contacts.keys()) if webpage_contacts else 'None'}")
        
        if webpage_contacts.get('emails'):
            # –î–æ–±–∞–≤–ª—è–µ–º –Ω–∞–π–¥–µ–Ω–Ω—ã–µ email –∞–¥—Ä–µ—Å–∞ –∫–∞–∫ –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–µ –∫–æ–Ω—Ç–∞–∫—Ç—ã
            processed['scientific_identifiers']['alternative_emails'] = webpage_contacts['emails'][:3]
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º ORCID –∏–∑ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö websites —Å —É–ª—É—á—à–µ–Ω–Ω—ã–º –∞–ª–≥–æ—Ä–∏—Ç–º–æ–º –≤—ã–±–æ—Ä–∞
        logger.info(f"INFO: –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ websites –≤ contact_information: {bool(webpage_contacts.get('websites'))}")
        if webpage_contacts.get('websites'):
            websites_list = webpage_contacts['websites']
            logger.info(f"INFO: –ü—Ä–æ–≤–µ—Ä—è–µ–º websites –≤ webpage_contacts: {len(websites_list)} —Å—Å—ã–ª–æ–∫ –Ω–∞–π–¥–µ–Ω–æ")
            
            # –õ–æ–≥–∏—Ä—É–µ–º –ø–µ—Ä–≤—ã–µ 10 —Å–∞–π—Ç–æ–≤ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
            logger.info(f"INFO: –ü–µ—Ä–≤—ã–µ 10 websites –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞:")
            for i, site in enumerate(websites_list[:10]):
                logger.info(f"INFO:   {i+1}. {site}")
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ ORCID –≤ –ø—Ä–∏–Ω—Ü–∏–ø–µ –≤ –¥–∞–Ω–Ω—ã—Ö
            orcid_count = sum(1 for site in websites_list if 'orcid' in str(site).lower())
            logger.info(f"INFO: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Å—ã–ª–æ–∫ —Å–æ–¥–µ—Ä–∂–∞—â–∏—Ö 'orcid': {orcid_count}")
            
            try:
                # –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ –≤–∞–ª–∏–¥–Ω—ã–µ ORCID —Å –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π
                logger.info(f"INFO: –í—ã–∑—ã–≤–∞–µ–º _extract_all_orcids_from_websites...")
                found_orcids = self._extract_all_orcids_from_websites(webpage_contacts['websites'])
                logger.info(f"INFO: _extract_all_orcids_from_websites –≤–µ—Ä–Ω—É–ª–∞: {len(found_orcids) if found_orcids else 0} ORCID")
                
                if found_orcids:
                    # –í—ã–±–∏—Ä–∞–µ–º –Ω–∞–∏–±–æ–ª–µ–µ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–π ORCID
                    logger.info(f"INFO: –í—ã–∑—ã–≤–∞–µ–º _select_best_orcid...")
                    best_orcid = self._select_best_orcid(found_orcids)
                    logger.info(f"INFO: –í—ã–±—Ä–∞–Ω –ª—É—á—à–∏–π ORCID –∏–∑ {len(found_orcids)} –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö: {best_orcid['orcid']} (—Ä–µ–π—Ç–∏–Ω–≥: {best_orcid['relevance_score']:.2f})")
                    processed['scientific_identifiers']['orcid_id'] = best_orcid['orcid']
                    
                    # –î–æ–±–∞–≤–ª—è–µ–º –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏
                    logger.info(f"INFO: –í—Å–µ –Ω–∞–π–¥–µ–Ω–Ω—ã–µ ORCID: {[o['orcid'] for o in found_orcids]}")
                else:
                    logger.info(f"INFO: –í–∞–ª–∏–¥–Ω—ã–µ ORCID –Ω–µ –Ω–∞–π–¥–µ–Ω—ã –≤ –≤–µ–±-–∞–Ω–∞–ª–∏–∑–µ")
            except Exception as e:
                logger.error(f"ERROR: –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ ORCID –∏–∑ –≤–µ–±-–∞–Ω–∞–ª–∏–∑–∞: {str(e)}")
                import traceback
                logger.error(f"ERROR: Traceback: {traceback.format_exc()}")
        
        # –û–±–Ω–æ–≤–ª—è–µ–º –∞–∫–∞–¥–µ–º–∏—á–µ—Å–∫—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é
        webpage_academic = webpage_analysis.get('academic_info', {})
        
        if webpage_academic.get('degrees'):
            processed['professional_info']['degrees'] = webpage_academic['degrees'][:3]
        
        if webpage_academic.get('research_areas'):
            # –û–±—ä–µ–¥–∏–Ω—è–µ–º —Å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–º–∏ –∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–º–∏ –∏–Ω—Ç–µ—Ä–µ—Å–∞–º–∏
            existing_interests = processed.get('research_interests', [])
            new_interests = webpage_academic['research_areas'][:5]
            
            # –ë–µ–∑–æ–ø–∞—Å–Ω–æ–µ —É–¥–∞–ª–µ–Ω–∏–µ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ –¥–ª—è –ª—é–±—ã—Ö —Ç–∏–ø–æ–≤ –¥–∞–Ω–Ω—ã—Ö
            combined_interests = []
            seen_items = []
            
            for item in existing_interests + new_interests:
                # –î–ª—è –ø—Ä–æ—Å—Ç—ã—Ö —Ç–∏–ø–æ–≤ –∏—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä—è–º–æ–µ —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ
                if isinstance(item, (str, int, float, bool)):
                    if item not in seen_items:
                        seen_items.append(item)
                        combined_interests.append(item)
                else:
                    # –î–ª—è —Å–ª–æ–∂–Ω—ã—Ö —Ç–∏–ø–æ–≤ —Å—Ä–∞–≤–Ω–∏–≤–∞–µ–º —Å—Ç—Ä–æ–∫–æ–≤–æ–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ
                    item_str = str(item)
                    if item_str not in [str(x) for x in combined_interests]:
                        combined_interests.append(item)
            
            processed['research_interests'] = combined_interests[:10]
        
        # –î–æ–±–∞–≤–ª—è–µ–º –≤—ã–≤–æ–¥—ã –Ω–∞ –æ—Å–Ω–æ–≤–µ –∞–Ω–∞–ª–∏–∑–∞ –≤–µ–±-—Å—Ç—Ä–∞–Ω–∏—Ü
        analysis_meta = webpage_analysis.get('analysis_metadata', {})
        successful_extractions = analysis_meta.get('successful_extractions', 0)
        
        if successful_extractions > 0:
            processed['conclusions'].append(
                f"–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–æ {successful_extractions} –≤–µ–±-—Å—Ç—Ä–∞–Ω–∏—Ü —Å –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π"
            )
            
            if webpage_analysis.get('owner_identification', {}).get('confidence_score', 0) > 0.5:
                processed['conclusions'].append(
                    "–í—ã—Å–æ–∫–∞—è –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ–π –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ü–∏–∏ –≤–ª–∞–¥–µ–ª—å—Ü–∞ email"
                )
    
    def _select_best_position(self, positions: List[str]) -> Optional[str]:
        """–í—ã–±–∏—Ä–∞–µ—Ç –Ω–∞–∏–±–æ–ª–µ–µ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—É—é –ø–æ–∑–∏—Ü–∏—é –∏–∑ —Å–ø–∏—Å–∫–∞"""
        
        if not positions:
            return None
        
        # –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç –∞–∫–∞–¥–µ–º–∏—á–µ—Å–∫–∏—Ö –∏ –Ω–∞—É—á–Ω—ã—Ö –ø–æ–∑–∏—Ü–∏–π
        priority_keywords = [
            '–ø—Ä–æ—Ñ–µ—Å—Å–æ—Ä', 'professor', '–¥–æ—Ü–µ–Ω—Ç', 'associate',
            '–¥–æ–∫—Ç–æ—Ä', 'doctor', '–∫–∞–Ω–¥–∏–¥–∞—Ç', 'candidate',
            '–¥–∏—Ä–µ–∫—Ç–æ—Ä', 'director', '–∑–∞–≤–µ–¥—É—é—â–∏–π', 'head'
        ]
        
        for keyword in priority_keywords:
            for position in positions:
                if keyword.lower() in position.lower():
                    return position
        
        # –ï—Å–ª–∏ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–Ω—ã—Ö –Ω–µ –Ω–∞–π–¥–µ–Ω–æ, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –ø–µ—Ä–≤—É—é
        return positions[0]
    
    def _select_best_organization(self, organizations: List[str]) -> Optional[str]:
        """–í—ã–±–∏—Ä–∞–µ—Ç –Ω–∞–∏–±–æ–ª–µ–µ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—É—é –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏—é –∏–∑ —Å–ø–∏—Å–∫–∞"""
        
        if not organizations:
            return None
        
        # –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç –∞–∫–∞–¥–µ–º–∏—á–µ—Å–∫–∏—Ö —É—á—Ä–µ–∂–¥–µ–Ω–∏–π
        priority_keywords = [
            '—É–Ω–∏–≤–µ—Ä—Å–∏—Ç–µ—Ç', 'university', '–∏–Ω—Å—Ç–∏—Ç—É—Ç', 'institute',
            '–∞–∫–∞–¥–µ–º–∏—è', 'academy', '–∫–æ–ª–ª–µ–¥–∂', 'college',
            '—Ü–µ–Ω—Ç—Ä', 'center', '–ª–∞–±–æ—Ä–∞—Ç–æ—Ä–∏—è', 'laboratory'
        ]
        
        for keyword in priority_keywords:
            for org in organizations:
                if keyword.lower() in org.lower() and len(org) > 10:  # –§–∏–ª—å—Ç—Ä—É–µ–º —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–µ –Ω–∞–∑–≤–∞–Ω–∏—è
                    return org
        
        # –í–æ–∑–≤—Ä–∞—â–∞–µ–º —Å–∞–º–æ–µ –¥–ª–∏–Ω–Ω–æ–µ –Ω–∞–∑–≤–∞–Ω–∏–µ (–æ–±—ã—á–Ω–æ –±–æ–ª–µ–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ç–∏–≤–Ω–æ–µ)
        return max(organizations, key=len) if organizations else None
    
    def _enhance_with_nlp_data(self, processed: Dict[str, Any], enhanced_nlp_results: Dict[str, Any]):
        """–û–±–æ–≥–∞—â–µ–Ω–∏–µ –æ—Å–Ω–æ–≤–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –¥–∞–Ω–Ω—ã–º–∏ –∏–∑ enhanced NLP –∞–Ω–∞–ª–∏–∑–∞"""
        
        try:
            enhanced_insights = enhanced_nlp_results.get('enhanced_insights', {})
            nlp_analysis = enhanced_nlp_results.get('nlp_analysis', {})
            
            # –û–±–Ω–æ–≤–ª—è–µ–º –æ—Å–Ω–æ–≤–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –≤–ª–∞–¥–µ–ª—å—Ü–µ –Ω–∞ –æ—Å–Ω–æ–≤–µ NLP –∞–Ω–∞–ª–∏–∑–∞
            # –ù–û —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –Ω–µ—Ç –±–æ–ª–µ–µ –≤—ã—Å–æ–∫–æ–π —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏ –æ—Ç enhanced verification
            most_confident_owner = enhanced_insights.get('most_confident_owner')
            existing_verification_confidence = processed['basic_info'].get('verification_confidence', 0)
            
            # Enhanced verification should take priority when verification confidence is high (>0.7)
            # This prevents NLP from overriding high-quality enhanced verification results
            should_use_nlp = (most_confident_owner and 
                             most_confident_owner.get('confidence', 0) > 0.6 and
                             existing_verification_confidence == 0 and  # No verification data yet
                             most_confident_owner.get('confidence', 0) > existing_verification_confidence)  # Higher confidence
            
            if should_use_nlp:
                processed['basic_info']['owner_name'] = most_confident_owner['name']
                processed['basic_info']['owner_name_en'] = self._transliterate_name(most_confident_owner['name'])
                processed['basic_info']['nlp_confidence'] = most_confident_owner['confidence']
                processed['basic_info']['nlp_source'] = most_confident_owner.get('source', 'nlp_analysis')
            elif most_confident_owner:  # –î–æ–±–∞–≤–ª—è–µ–º NLP –¥–∞–Ω–Ω—ã–µ –¥–∞–∂–µ –µ—Å–ª–∏ –Ω–µ –æ–±–Ω–æ–≤–ª—è–µ–º –∏–º—è
                processed['basic_info']['nlp_confidence'] = most_confident_owner['confidence']
                processed['basic_info']['nlp_source'] = most_confident_owner.get('source', 'nlp_analysis')
        
            
            # –û–±–Ω–æ–≤–ª—è–µ–º –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é
            professional_context = enhanced_insights.get('professional_context', {})
            if professional_context:
                # –û–±–Ω–æ–≤–ª—è–µ–º —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—é –Ω–∞ –æ—Å–Ω–æ–≤–µ NLP –∞–Ω–∞–ª–∏–∑–∞ –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã—Ö —Ä–æ–ª–µ–π
                if professional_context.get('primary_category'):
                    category_mapping = {
                        'academic': '–ê–∫–∞–¥–µ–º–∏—á–µ—Å–∫–∞—è –¥–µ—è—Ç–µ–ª—å–Ω–æ—Å—Ç—å',
                        'medical': '–ú–µ–¥–∏—Ü–∏–Ω–∞ –∏ –∑–¥—Ä–∞–≤–æ–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ',
                        'technical': '–¢–µ—Ö–Ω–∏—á–µ—Å–∫–∞—è –¥–µ—è—Ç–µ–ª—å–Ω–æ—Å—Ç—å',
                        'management': '–£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –∏ –∞–¥–º–∏–Ω–∏—Å—Ç—Ä–∏—Ä–æ–≤–∞–Ω–∏–µ'
                    }
                    category = professional_context['primary_category']
                    if category in category_mapping:
                        processed['professional_info']['specialization'] = category_mapping[category]
                
                # –î–æ–±–∞–≤–ª—è–µ–º –Ω–∞–π–¥–µ–Ω–Ω—ã–µ —Ä–æ–ª–∏
                roles_found = professional_context.get('roles_found', [])
                if roles_found:
                    processed['professional_info']['nlp_roles'] = roles_found[:3]
            
            # –û–±–Ω–æ–≤–ª—è–µ–º –Ω–∞—É—á–Ω—ã–µ –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä—ã
            if nlp_analysis.get('entities_found'):
                # –ò—â–µ–º –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–∏ –≤ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö —Å—É—â–Ω–æ—Å—Ç—è—Ö
                organizations = [e['text'] for e in nlp_analysis['entities_found'] 
                               if e['label'] in ['ORG', 'ORGANIZATION'] and e['confidence'] > 0.7]
                if organizations:
                    processed['scientific_identifiers']['nlp_organizations'] = organizations[:2]
                
                # –ò—â–µ–º –ø–µ—Ä—Å–æ–Ω –≤ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö —Å—É—â–Ω–æ—Å—Ç—è—Ö
                persons = [e['text'] for e in nlp_analysis['entities_found'] 
                          if e['label'] == 'PERSON' and e['confidence'] > 0.7]
                if persons:
                    processed['scientific_identifiers']['nlp_persons'] = persons[:3]
            
            # –û–±–Ω–æ–≤–ª—è–µ–º –æ–±–ª–∞—Å—Ç–∏ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–π –Ω–∞ –æ—Å–Ω–æ–≤–µ NLP –∞–Ω–∞–ª–∏–∑–∞
            professional_roles = nlp_analysis.get('professional_roles', [])
            if professional_roles:
                role_based_interests = []
                for role in professional_roles:
                    if role['confidence'] > 0.6:
                        # –°–æ–∑–¥–∞–µ–º –æ–±–ª–∞—Å—Ç—å –∏–Ω—Ç–µ—Ä–µ—Å–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ä–æ–ª–∏
                        interest = f"{role['title']} ({role['category']})"
                        role_based_interests.append(interest)
                
                if role_based_interests:
                    existing_interests = processed.get('research_interests', [])
                    combined_interests = list(set(existing_interests + role_based_interests))
                    processed['research_interests'] = combined_interests[:12]
            
            # –î–æ–±–∞–≤–ª—è–µ–º –≤—ã–≤–æ–¥—ã –Ω–∞ –æ—Å–Ω–æ–≤–µ NLP –∞–Ω–∞–ª–∏–∑–∞
            confidence_scores = nlp_analysis.get('confidence_scores', {})
            overall_confidence = confidence_scores.get('overall', 0)
            
            if overall_confidence > 0.7:
                processed['conclusions'].append(
                    "NLP –∞–Ω–∞–ª–∏–∑ –ø–æ–∫–∞–∑–∞–ª –≤—ã—Å–æ–∫—É—é –¥–æ—Å—Ç–æ–≤–µ—Ä–Ω–æ—Å—Ç—å –∏–∑–≤–ª–µ—á–µ–Ω–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏"
                )
            elif overall_confidence > 0.4:
                processed['conclusions'].append(
                    "NLP –∞–Ω–∞–ª–∏–∑ –ø–æ–∫–∞–∑–∞–ª —É–º–µ—Ä–µ–Ω–Ω—É—é –¥–æ—Å—Ç–æ–≤–µ—Ä–Ω–æ—Å—Ç—å –∏–∑–≤–ª–µ—á–µ–Ω–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏"
                )
            
            # –î–æ–±–∞–≤–ª—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏—Ö –ø–∞—Ç—Ç–µ—Ä–Ω–∞—Ö
            semantic_patterns = nlp_analysis.get('semantic_patterns', [])
            if 'academic_professional' in semantic_patterns:
                processed['conclusions'].append(
                    "–û–±–Ω–∞—Ä—É–∂–µ–Ω—ã –ø—Ä–∏–∑–Ω–∞–∫–∏ –∞–∫–∞–¥–µ–º–∏—á–µ—Å–∫–æ–π –¥–µ—è—Ç–µ–ª—å–Ω–æ—Å—Ç–∏"
                )
            if 'medical_professional' in semantic_patterns:
                processed['conclusions'].append(
                    "–û–±–Ω–∞—Ä—É–∂–µ–Ω—ã –ø—Ä–∏–∑–Ω–∞–∫–∏ –º–µ–¥–∏—Ü–∏–Ω—Å–∫–æ–π –¥–µ—è—Ç–µ–ª—å–Ω–æ—Å—Ç–∏"
                )
            if 'research_activity' in semantic_patterns:
                processed['conclusions'].append(
                    "–û–±–Ω–∞—Ä—É–∂–µ–Ω—ã –ø—Ä–∏–∑–Ω–∞–∫–∏ –∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å—Å–∫–æ–π –¥–µ—è—Ç–µ–ª—å–Ω–æ—Å—Ç–∏"
                )
            
            # –û–±–Ω–æ–≤–ª—è–µ–º –æ—Ü–µ–Ω–∫—É –Ω–∞–¥–µ–∂–Ω–æ—Å—Ç–∏ –∫–æ–Ω—Ç–∞–∫—Ç–∞
            contact_reliability = enhanced_insights.get('contact_reliability', 'unknown')
            reliability_mapping = {
                'high': '–í—ã—Å–æ–∫–∞—è –Ω–∞–¥–µ–∂–Ω–æ—Å—Ç—å –∫–æ–Ω—Ç–∞–∫—Ç–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏',
                'medium': '–£–º–µ—Ä–µ–Ω–Ω–∞—è –Ω–∞–¥–µ–∂–Ω–æ—Å—Ç—å –∫–æ–Ω—Ç–∞–∫—Ç–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏',
                'low': '–ù–∏–∑–∫–∞—è –Ω–∞–¥–µ–∂–Ω–æ—Å—Ç—å –∫–æ–Ω—Ç–∞–∫—Ç–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏'
            }
            
            if contact_reliability in reliability_mapping:
                processed['conclusions'].append(reliability_mapping[contact_reliability])
            
            # –î–æ–±–∞–≤–ª—è–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –æ NLP –∞–Ω–∞–ª–∏–∑–µ
            methods_used = enhanced_nlp_results.get('metadata', {}).get('methods_used', [])
            if methods_used:
                processed['conclusions'].append(
                    f"–ü—Ä–∏–º–µ–Ω–µ–Ω—ã –º–µ—Ç–æ–¥—ã NLP –∞–Ω–∞–ª–∏–∑–∞: {', '.join(methods_used)}"
                )
            
            logger.info("–û—Å–Ω–æ–≤–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è —É—Å–ø–µ—à–Ω–æ –æ–±–æ–≥–∞—â–µ–Ω–∞ –¥–∞–Ω–Ω—ã–º–∏ NLP –∞–Ω–∞–ª–∏–∑–∞")
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±–æ–≥–∞—â–µ–Ω–∏–∏ –¥–∞–Ω–Ω—ã–º–∏ NLP –∞–Ω–∞–ª–∏–∑–∞: {e}")
            processed['conclusions'].append(
                "–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–∏ NLP –∞–Ω–∞–ª–∏–∑–∞ –∫ –∏–∑–≤–ª–µ—á–µ–Ω–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏"
            )
    
    def _prepare_data_for_verifier(self, webpage_analysis: Dict[str, Any], search_results: List[Dict]) -> List[Dict[str, Any]]:
        """–ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ –¥–ª—è Enhanced Parser Verifier"""
        
        parsed_results = []
        
        try:
            # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –¥–∞–Ω–Ω—ã–µ –∞–Ω–∞–ª–∏–∑–∞ –≤–µ–±-—Å—Ç—Ä–∞–Ω–∏—Ü –≤ —Ñ–æ—Ä–º–∞—Ç –¥–ª—è –≤–µ—Ä–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞
            if webpage_analysis:
                # –°–æ–∑–¥–∞–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä—É –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∫–∞–∂–¥–æ–π –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã
                analyzed_urls = webpage_analysis.get('analysis_metadata', {}).get('analyzed_urls', [])
                
                for url_data in analyzed_urls:
                    if url_data.get('status') == 'success':
                        result_entry = {
                            'url': url_data['url'],
                            'metadata': {
                                'url': url_data['url'],
                                'status': 'success',
                                'extraction_types': url_data.get('extracted_data_types', [])
                            },
                            'names': [],
                            'organizations': [],
                            'positions': [],
                            'contact_info': {'emails': []}
                        }
                        
                        # –î–æ–±–∞–≤–ª—è–µ–º –Ω–∞–π–¥–µ–Ω–Ω—ã–µ –∏–º–µ–Ω–∞
                        owner_names = webpage_analysis.get('owner_identification', {}).get('names_found', [])
                        result_entry['names'] = owner_names[:5]  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ
                        
                        # –î–æ–±–∞–≤–ª—è–µ–º –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–∏
                        organizations = webpage_analysis.get('professional_details', {}).get('organizations', [])
                        result_entry['organizations'] = organizations[:5]
                        
                        # –î–æ–±–∞–≤–ª—è–µ–º –ø–æ–∑–∏—Ü–∏–∏
                        positions = webpage_analysis.get('professional_details', {}).get('positions', [])
                        result_entry['positions'] = positions[:5]
                        
                        # –î–æ–±–∞–≤–ª—è–µ–º email –∞–¥—Ä–µ—Å–∞
                        emails = webpage_analysis.get('contact_information', {}).get('emails', [])
                        result_entry['contact_info']['emails'] = emails[:5]
                        
                        parsed_results.append(result_entry)
            
            # –î–æ–±–∞–≤–ª—è–µ–º –¥–∞–Ω–Ω—ã–µ –∏–∑ –ø–æ–∏—Å–∫–æ–≤—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤, –µ—Å–ª–∏ –æ–Ω–∏ –Ω–µ –ø–æ–∫—Ä—ã—Ç—ã –∞–Ω–∞–ª–∏–∑–æ–º –≤–µ–±-—Å—Ç—Ä–∞–Ω–∏—Ü
            if len(parsed_results) == 0 and search_results:
                for search_result in search_results[:5]:  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ
                    result_entry = {
                        'url': search_result.get('link', ''),
                        'metadata': {
                            'url': search_result.get('link', ''),
                            'title': search_result.get('title', ''),
                            'snippet': search_result.get('snippet', ''),
                            'status': 'search_result'
                        },
                        'names': self._extract_names_from_text(search_result.get('title', '') + ' ' + search_result.get('snippet', '')),
                        'organizations': self._extract_organizations_from_text(search_result.get('title', '') + ' ' + search_result.get('snippet', '')),
                        'positions': self._extract_positions_from_text(search_result.get('title', '') + ' ' + search_result.get('snippet', '')),
                        'contact_info': {'emails': []}
                    }
                    parsed_results.append(result_entry)
            
            logger.info(f"–ü–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–æ {len(parsed_results)} –∑–∞–ø–∏—Å–µ–π –¥–ª—è enhanced –≤–µ—Ä–∏—Ñ–∏–∫–∞—Ü–∏–∏")
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–µ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –≤–µ—Ä–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞: {e}")
        
        return parsed_results
    
    def _integrate_verified_data(self, processed: Dict[str, Any], enhanced_profile: Dict[str, Any]):
        """–ò–Ω—Ç–µ–≥—Ä–∏—Ä—É–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã enhanced –≤–µ—Ä–∏—Ñ–∏–∫–∞—Ü–∏–∏ –≤ –æ—Å–Ω–æ–≤–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ"""
        
        try:
            verified_data = enhanced_profile.get('verified_data', {})
            
            # –û–±–Ω–æ–≤–ª—è–µ–º –∏–º–µ–Ω–∞ —Å –≤—ã—Å–æ–∫–æ–π —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å—é
            logger.info(f"DEBUG: Enhanced verification - –ø—Ä–æ–≤–µ—Ä—è–µ–º verified_data: {list(verified_data.keys())}")
            
            if 'names' in verified_data:
                name_data = verified_data['names']
                logger.info(f"DEBUG: Enhanced verification - name_data: {name_data}")
                if name_data.get('confidence', 0) > 0.5 and name_data.get('value'):
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â—É—é —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –∏–∑ webpage analysis
                    existing_confidence = processed['basic_info'].get('confidence_score', 0)
                    verification_confidence = name_data['confidence']
                    
                    # Enhanced verification –≤—Å–µ–≥–¥–∞ –∏–º–µ–µ—Ç –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç –ø—Ä–∏ confidence > 0.5
                    # –∏–ª–∏ –∫–æ–≥–¥–∞ –µ–≥–æ confidence –≤—ã—à–µ, —á–µ–º —É webpage analysis
                    logger.info(f"DEBUG: –ü—Ä–æ–≤–µ—Ä—è–µ–º enhanced verification: verification_confidence={verification_confidence}, existing_confidence={existing_confidence}")
                    logger.info(f"DEBUG: –¢–µ–∫—É—â–∏–π owner_name –ü–ï–†–ï–î –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ–º: {processed['basic_info'].get('owner_name', 'None')}")
                    
                    if verification_confidence > 0.5 or verification_confidence > existing_confidence:
                        old_name = processed['basic_info'].get('owner_name', 'None')
                        processed['basic_info']['owner_name'] = name_data['value']
                        processed['basic_info']['owner_name_en'] = self._transliterate_name(name_data['value'])
                        processed['basic_info']['verification_confidence'] = verification_confidence
                        processed['basic_info']['verification_sources'] = name_data.get('source_count', 0)
                        # –û–±–Ω–æ–≤–ª—è–µ–º –æ–±—â—É—é —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –Ω–∞ enhanced verification confidence
                        processed['basic_info']['confidence_score'] = verification_confidence
                        
                        logger.info(f"DEBUG: –û–ë–ù–û–í–õ–ï–ù–û –∏–º—è –≤–ª–∞–¥–µ–ª—å—Ü–∞: '{old_name}' -> '{name_data['value']}' (confidence: {name_data['confidence']:.2f})")
                        logger.info(f"DEBUG: –ü–û–°–õ–ï –û–ë–ù–û–í–õ–ï–ù–ò–Ø: owner_name = '{processed['basic_info']['owner_name']}'")
                    else:
                        logger.info(f"DEBUG: –ù–ï –æ–±–Ω–æ–≤–ª—è–µ–º –∏–º—è, —Ç–∞–∫ –∫–∞–∫ verification_confidence={verification_confidence} <= 0.5 –∏ <= existing_confidence={existing_confidence}")
            
            # –û–±–Ω–æ–≤–ª—è–µ–º –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–∏
            if 'organizations' in verified_data:
                org_data = verified_data['organizations']
                if org_data.get('confidence', 0) > 0.4 and org_data.get('value'):
                    processed['professional_info']['workplace'] = org_data['value']
                    processed['professional_info']['workplace_confidence'] = org_data['confidence']
            
            # –û–±–Ω–æ–≤–ª—è–µ–º –ø–æ–∑–∏—Ü–∏–∏
            if 'positions' in verified_data:
                pos_data = verified_data['positions']
                if pos_data.get('confidence', 0) > 0.4 and pos_data.get('value'):
                    processed['professional_info']['position'] = pos_data['value']
                    processed['professional_info']['position_confidence'] = pos_data['confidence']
            
            # –û–±–Ω–æ–≤–ª—è–µ–º email –∞–¥—Ä–µ—Å–∞
            if 'emails' in verified_data:
                email_data = verified_data['emails']
                if email_data.get('confidence', 0) > 0.5 and email_data.get('value'):
                    processed['scientific_identifiers']['verified_email'] = email_data['value']
                    processed['scientific_identifiers']['email_confidence'] = email_data['confidence']
            
            # –î–æ–±–∞–≤–ª—è–µ–º –º–µ—Ç—Ä–∏–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞
            quality_metrics = enhanced_profile.get('data_quality_metrics', {})
            if quality_metrics:
                processed['verification_quality'] = {
                    'overall_quality': quality_metrics.get('overall_quality', 0),
                    'average_confidence': quality_metrics.get('average_confidence', 0),
                    'data_coverage': quality_metrics.get('data_coverage', 0),
                    'consistency_score': quality_metrics.get('consistency_score', 0)
                }
            
            # –î–æ–±–∞–≤–ª—è–µ–º –ø—Ä–æ–≤–µ—Ä–∫–∏ –∫–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ç–Ω–æ—Å—Ç–∏
            consistency_checks = enhanced_profile.get('consistency_checks', {})
            if consistency_checks:
                processed['consistency_analysis'] = consistency_checks
            
            # –î–æ–±–∞–≤–ª—è–µ–º —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
            recommendations = enhanced_profile.get('recommendations', [])
            if recommendations:
                for rec in recommendations[:3]:  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ
                    processed['conclusions'].append(f"–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è: {rec}")
            
            # –î–æ–±–∞–≤–ª—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –º–µ—Ç–æ–¥–µ –≤–µ—Ä–∏—Ñ–∏–∫–∞—Ü–∏–∏
            processed['conclusions'].append("–ü—Ä–∏–º–µ–Ω–µ–Ω–∞ enhanced –≤–µ—Ä–∏—Ñ–∏–∫–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º fuzzy matching")
            
            logger.info("Enhanced –≤–µ—Ä–∏—Ñ–∏–∫–∞—Ü–∏—è —É—Å–ø–µ—à–Ω–æ –∏–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞–Ω–∞ –≤ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã")
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ enhanced –≤–µ—Ä–∏—Ñ–∏–∫–∞—Ü–∏–∏: {e}")
    
    def _extract_names_from_text(self, text: str) -> List[str]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –∏–º–µ–Ω–∞ –∏–∑ —Ç–µ–∫—Å—Ç–∞"""
        names = []
        
        # –ü–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è –ø–æ–∏—Å–∫–∞ –∏–º–µ–Ω
        name_patterns = [
            r'([–ê-–Ø][–∞-—è]+\s+[–ê-–Ø][–∞-—è]+\s+[–ê-–Ø][–∞-—è]+)',  # –†—É—Å—Å–∫–∏–µ –§–ò–û
            r'([A-Z][a-z]+\s+[A-Z][a-z]+)',  # –ê–Ω–≥–ª–∏–π—Å–∫–∏–µ –∏–º–µ–Ω–∞
            r'([–ê-–Ø]\.[–ê-–Ø]\.\s+[–ê-–Ø][–∞-—è]+)',  # –ò–Ω–∏—Ü–∏–∞–ª—ã + —Ñ–∞–º–∏–ª–∏—è
            r'([–ê-–Ø][–∞-—è]+\s+[–ê-–Ø]\.[–ê-–Ø]\.)',  # –§–∞–º–∏–ª–∏—è + –∏–Ω–∏—Ü–∏–∞–ª—ã
        ]
        
        for pattern in name_patterns:
            matches = re.findall(pattern, text)
            names.extend(matches)
        
        return list(set(names))  # –£–¥–∞–ª—è–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã
    
    def _extract_organizations_from_text(self, text: str) -> List[str]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–∏ –∏–∑ —Ç–µ–∫—Å—Ç–∞"""
        organizations = []
        
        # –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –¥–ª—è –ø–æ–∏—Å–∫–∞ –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–π
        org_patterns = [
            r'([–ê-–Ø][–∞-—è]*\s*(?:–≥–æ—Å—É–¥–∞—Ä—Å—Ç–≤–µ–Ω–Ω—ã–π|–º–µ–¥–∏—Ü–∏–Ω—Å–∫–∏–π|—Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–π|–ø–µ–¥–∞–≥–æ–≥–∏—á–µ—Å–∫–∏–π)\s*(?:—É–Ω–∏–≤–µ—Ä—Å–∏—Ç–µ—Ç|–∏–Ω—Å—Ç–∏—Ç—É—Ç|–∞–∫–∞–¥–µ–º–∏—è|—Ü–µ–Ω—Ç—Ä))',
            r'([A-Z][a-z]*\s*(?:University|Institute|Academy|Center|College))',
            r'(\b[–ê-–Ø][–∞-—è]+\s+[–∞-—è]*—É–Ω–∏–≤–µ—Ä—Å–∏—Ç–µ—Ç\b)',
            r'(\b[A-Z][a-z]+\s+University\b)'
        ]
        
        for pattern in org_patterns:
            matches = re.findall(pattern, text, re.IGNORECASE)
            organizations.extend(matches)
        
        return list(set(organizations))
    
    def _extract_positions_from_text(self, text: str) -> List[str]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –¥–æ–ª–∂–Ω–æ—Å—Ç–∏ –∏–∑ —Ç–µ–∫—Å—Ç–∞"""
        positions = []
        
        # –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –¥–ª—è –¥–æ–ª–∂–Ω–æ—Å—Ç–µ–π
        position_keywords = [
            '–ø—Ä–æ—Ñ–µ—Å—Å–æ—Ä', 'professor', '–¥–æ—Ü–µ–Ω—Ç', 'associate professor',
            '–∫–∞–Ω–¥–∏–¥–∞—Ç –Ω–∞—É–∫', '–¥–æ–∫—Ç–æ—Ä –Ω–∞—É–∫', 'PhD', 'MD',
            '–∑–∞–≤–µ–¥—É—é—â–∏–π –∫–∞—Ñ–µ–¥—Ä–æ–π', 'head of department',
            '–¥–∏—Ä–µ–∫—Ç–æ—Ä', 'director', '—Ä—É–∫–æ–≤–æ–¥–∏—Ç–µ–ª—å', 'manager'
        ]
        
        text_lower = text.lower()
        
        for keyword in position_keywords:
            if keyword.lower() in text_lower:
                # –ò–∑–≤–ª–µ–∫–∞–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç –≤–æ–∫—Ä—É–≥ –∫–ª—é—á–µ–≤–æ–≥–æ —Å–ª–æ–≤–∞
                context = self._extract_context(text_lower, keyword.lower(), 20)
                if context:
                    positions.append(context.strip())
        
        return list(set(positions))
    
    
    def _enhance_with_nlp_data(self, processed: Dict[str, Any], nlp_data: Dict[str, Any]):
        """–û–±–æ–≥–∞—â–µ–Ω–∏–µ –æ—Å–Ω–æ–≤–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –¥–∞–Ω–Ω—ã–º–∏ –∏–∑ NLP –∞–Ω–∞–ª–∏–∑–∞"""
        
        try:
            # –û–±–Ω–æ–≤–ª—è–µ–º –æ—Å–Ω–æ–≤–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –≤–ª–∞–¥–µ–ª—å—Ü–µ –Ω–∞ –æ—Å–Ω–æ–≤–µ NLP –∞–Ω–∞–ª–∏–∑–∞
            enhanced_insights = nlp_data.get('enhanced_insights', {})
            most_confident_owner = enhanced_insights.get('most_confident_owner')
            
            if most_confident_owner and most_confident_owner.get('confidence', 0) > 0.6:
                # –¢–æ–ª—å–∫–æ –æ–±–Ω–æ–≤–ª—è–µ–º –µ—Å–ª–∏ —É –Ω–∞—Å –µ—â–µ –Ω–µ—Ç –∏–º–µ–Ω–∏ –∏–ª–∏ NLP –∏–º–µ–µ—Ç –±–æ–ª–µ–µ –≤—ã—Å–æ–∫—É—é —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å
                current_confidence = processed['basic_info'].get('confidence_score', 0)
                if processed['basic_info']['owner_name'] == '–ù–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–æ' or most_confident_owner['confidence'] > current_confidence:
                    processed['basic_info']['owner_name'] = most_confident_owner['name']
                    processed['basic_info']['owner_name_en'] = self._transliterate_name(most_confident_owner['name'])
                    processed['basic_info']['confidence_score'] = most_confident_owner['confidence']
                    processed['basic_info']['nlp_confidence'] = most_confident_owner['confidence']
                    processed['basic_info']['nlp_source'] = most_confident_owner.get('source', 'nlp_analysis')
            
            # –û–±–Ω–æ–≤–ª—è–µ–º –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é
            professional_context = enhanced_insights.get('professional_context', {})
            if professional_context.get('primary_category'):
                category_mapping = {
                    'academic': '–ê–∫–∞–¥–µ–º–∏—á–µ—Å–∫–∞—è –¥–µ—è—Ç–µ–ª—å–Ω–æ—Å—Ç—å',
                    'medical': '–ú–µ–¥–∏—Ü–∏–Ω–∞ –∏ –∑–¥—Ä–∞–≤–æ–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ', 
                    'technical': '–¢–µ—Ö–Ω–∏—á–µ—Å–∫–∞—è –¥–µ—è—Ç–µ–ª—å–Ω–æ—Å—Ç—å',
                    'management': '–£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –∏ –∞–¥–º–∏–Ω–∏—Å—Ç—Ä–∏—Ä–æ–≤–∞–Ω–∏–µ'
                }
                category = professional_context['primary_category']
                if category in category_mapping:
                    processed['professional_info']['specialization'] = category_mapping[category]
            
            # –î–æ–±–∞–≤–ª—è–µ–º –≤—ã–≤–æ–¥—ã –Ω–∞ –æ—Å–Ω–æ–≤–µ NLP –∞–Ω–∞–ª–∏–∑–∞
            nlp_analysis = nlp_data.get('nlp_analysis', {})
            confidence_scores = nlp_analysis.get('confidence_scores', {})
            overall_confidence = confidence_scores.get('overall', 0)
            
            if overall_confidence > 0.7:
                processed['conclusions'].append("NLP –∞–Ω–∞–ª–∏–∑ –ø–æ–∫–∞–∑–∞–ª –≤—ã—Å–æ–∫—É—é –¥–æ—Å—Ç–æ–≤–µ—Ä–Ω–æ—Å—Ç—å –∏–∑–≤–ª–µ—á–µ–Ω–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏")
            elif overall_confidence > 0.4:
                processed['conclusions'].append("NLP –∞–Ω–∞–ª–∏–∑ –ø–æ–∫–∞–∑–∞–ª —É–º–µ—Ä–µ–Ω–Ω—É—é –¥–æ—Å—Ç–æ–≤–µ—Ä–Ω–æ—Å—Ç—å –∏–∑–≤–ª–µ—á–µ–Ω–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏")
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±–æ–≥–∞—â–µ–Ω–∏–∏ –¥–∞–Ω–Ω—ã–º–∏ NLP –∞–Ω–∞–ª–∏–∑–∞: {e}")
    
    def _generate_conclusions(self, email: str, search_results: List[Dict]) -> List[str]:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –≤—ã–≤–æ–¥–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ –Ω–∞–π–¥–µ–Ω–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏"""
        
        conclusions = []
        
        if len(search_results) > 10:
            conclusions.append("Email –ø—Ä–∏–Ω–∞–¥–ª–µ–∂–∏—Ç –∞–∫—Ç–∏–≤–Ω–æ–º—É –∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—é —Å –≤—ã—Å–æ–∫–æ–π –æ–Ω–ª–∞–π–Ω-–∞–∫—Ç–∏–≤–Ω–æ—Å—Ç—å—é")
        elif len(search_results) > 5:
            conclusions.append("Email –ø—Ä–∏–Ω–∞–¥–ª–µ–∂–∏—Ç —Å–ø–µ—Ü–∏–∞–ª–∏—Å—Ç—É —Å —É–º–µ—Ä–µ–Ω–Ω–æ–π –æ–Ω–ª–∞–π–Ω-–∞–∫—Ç–∏–≤–Ω–æ—Å—Ç—å—é")
        else:
            conclusions.append("–û–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –≤–ª–∞–¥–µ–ª—å—Ü–µ email")
        
        # –ê–Ω–∞–ª–∏–∑ –¥–æ–º–µ–Ω–æ–≤ –≤ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞—Ö
        academic_domains = ['edu', 'ac', 'university', 'institute']
        has_academic = any(domain in result.get('link', '') for result in search_results for domain in academic_domains)
        
        if has_academic:
            conclusions.append("–°–≤—è–∑–∞–Ω —Å –∞–∫–∞–¥–µ–º–∏—á–µ—Å–∫–∏–º–∏ —É—á—Ä–µ–∂–¥–µ–Ω–∏—è–º–∏")
        
        return conclusions
    
    def _extract_sources(self, search_results: List[Dict]) -> List[str]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏"""
        
        sources = []
        for result in search_results:
            link = result.get('link', '')
            if link:
                domain = self._extract_domain(link)
                if domain not in sources:
                    sources.append(domain)
        
        return sources
    
    def _extract_publications(self, search_results: List[Dict]) -> List[Dict]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –ø—É–±–ª–∏–∫–∞—Ü–∏—è—Ö"""
        
        publications = []
        
        for result in search_results:
            if any(keyword in result.get('title', '').lower() for keyword in ['journal', 'article', 'publication', 'research']):
                publications.append({
                    'title': result.get('title', ''),
                    'journal': '–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç—Å—è –∏–∑ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞',
                    'year': '–ù–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω',
                    'authors': '–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç—Å—è',
                    'doi': '–ù–µ –Ω–∞–π–¥–µ–Ω',
                    'url': result.get('link', '')
                })
        
        return publications[:5]  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ
    
    def _extract_research_interests(self, search_results: List[Dict]) -> List[str]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –æ–±–ª–∞—Å—Ç–µ–π –Ω–∞—É—á–Ω—ã—Ö –∏–Ω—Ç–µ—Ä–µ—Å–æ–≤"""
        
        interests = []
        keywords = ['–∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ', '–∏–∑—É—á–µ–Ω–∏–µ', '–∞–Ω–∞–ª–∏–∑', '—Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∞', '–º–µ—Ç–æ–¥—ã', '—Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏']
        
        for result in search_results:
            text = f"{result.get('title', '')} {result.get('snippet', '')}".lower()
            for keyword in keywords:
                if keyword in text:
                    context = self._extract_context(text, keyword, 30)
                    if context not in interests:
                        interests.append(context)
        
        return interests[:10]  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ
    
    def _extract_all_orcids_from_websites(self, websites: List[str]) -> List[Dict[str, Any]]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –≤—Å–µ –≤–∞–ª–∏–¥–Ω—ã–µ ORCID –∏–∑ —Å–ø–∏—Å–∫–∞ websites —Å –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π –¥–ª—è —Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏—è"""
        import re
        
        orcid_pattern = re.compile(r'https?://orcid\.org/(?:0000-)?([0-9]{4}-[0-9]{4}-[0-9]{4}-[0-9X]{4})', re.IGNORECASE)
        found_orcids = []
        
        for index, website in enumerate(websites):
            if isinstance(website, str):
                # –£–±–∏—Ä–∞–µ–º –≤–æ–∑–º–æ–∂–Ω—ã–µ –∑–∞–ø—è—Ç—ã–µ –≤ –∫–æ–Ω—Ü–µ URL
                clean_url = website.rstrip(',')
                logger.info(f"INFO: –ü—Ä–æ–≤–µ—Ä—è–µ–º website #{index+1}: {clean_url}")
                
                match = orcid_pattern.search(clean_url)
                if match:
                    orcid_id = f"0000-{match.group(1)}" if not match.group(1).startswith('0000-') else match.group(1)
                    
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ—Å—Ç—å —Ñ–æ—Ä–º–∞—Ç–∞ ORCID
                    if re.match(r'^0000-[0-9]{4}-[0-9]{4}-[0-9X]{4}$', orcid_id):
                        logger.info(f"INFO: –ù–∞–π–¥–µ–Ω –≤–∞–ª–∏–¥–Ω—ã–π ORCID: {orcid_id} –Ω–∞ –ø–æ–∑–∏—Ü–∏–∏ {index+1}")
                        
                        orcid_info = {
                            'orcid': orcid_id,
                            'url': clean_url,
                            'position_in_list': index,  # –ü–æ–∑–∏—Ü–∏—è –≤ –∏—Å—Ö–æ–¥–Ω–æ–º —Å–ø–∏—Å–∫–µ
                            'relevance_score': 0.0,     # –ë—É–¥–µ—Ç —Ä–∞—Å—Å—á–∏—Ç–∞–Ω –≤ _calculate_orcid_relevance
                            'is_complete_url': clean_url.startswith('http'),
                            'domain_context': self._extract_domain_context(clean_url)
                        }
                        
                        # –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å
                        orcid_info['relevance_score'] = self._calculate_orcid_relevance(orcid_info, index, websites)
                        
                        found_orcids.append(orcid_info)
                    else:
                        logger.info(f"INFO: –ù–∞–π–¥–µ–Ω ORCID —Å –Ω–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–º —Ñ–æ—Ä–º–∞—Ç–æ–º: {orcid_id}")
                else:
                    logger.info(f"INFO: ORCID –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ URL: {clean_url}")
        
        logger.info(f"INFO: –í—Å–µ–≥–æ –Ω–∞–π–¥–µ–Ω–æ {len(found_orcids)} –≤–∞–ª–∏–¥–Ω—ã—Ö ORCID")
        return found_orcids
    
    def _calculate_orcid_relevance(self, orcid_info: Dict[str, Any], position: int, all_websites: List[str]) -> float:
        """–†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ—Ç —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å ORCID –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Ñ–∞–∫—Ç–æ—Ä–æ–≤"""
        
        relevance_score = 0.0
        
        # 1. –ë–æ–Ω—É—Å –∑–∞ –ø–æ–∑–∏—Ü–∏—é –≤ —Å–ø–∏—Å–∫–µ (—á–µ–º —Ä–∞–Ω—å—à–µ, —Ç–µ–º –ª—É—á—à–µ)
        position_bonus = max(0, (len(all_websites) - position) / len(all_websites)) * 0.3
        relevance_score += position_bonus
        
        # 2. –ë–æ–Ω—É—Å –∑–∞ –ø–æ–ª–Ω—ã–π URL (vs –Ω–µ–ø–æ–ª–Ω—ã–π)
        if orcid_info['is_complete_url']:
            relevance_score += 0.2
        
        # 3. –ë–æ–Ω—É—Å –∑–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç –¥–æ–º–µ–Ω–∞
        domain_context = orcid_info['domain_context']
        if domain_context:
            # –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç –∞–∫–∞–¥–µ–º–∏—á–µ—Å–∫–∏–º –∏ –∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–º –¥–æ–º–µ–Ω–∞–º
            academic_domains = ['edu', 'ac.', 'university', 'institute', 'research', 'ncbi', 'pubmed', 'scholar']
            if any(domain in domain_context.lower() for domain in academic_domains):
                relevance_score += 0.3
            
            # –ë–æ–Ω—É—Å –∑–∞ –∏–∑–≤–µ—Å—Ç–Ω—ã–µ –Ω–∞—É—á–Ω—ã–µ –ø–ª–∞—Ç—Ñ–æ—Ä–º—ã
            scientific_platforms = ['orcid.org', 'researchgate', 'academia.edu', 'ieee', 'springer', 'elsevier']
            if any(platform in domain_context.lower() for platform in scientific_platforms):
                relevance_score += 0.2
        
        # 4. –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã –≤ URL
        url = orcid_info['url'].lower()
        
        # –ë–æ–Ω—É—Å –∑–∞ –ø—Ä—è–º—ã–µ —Å—Å—ã–ª–∫–∏ –Ω–∞ ORCID –ø—Ä–æ—Ñ–∏–ª—å
        if 'orcid.org' in url and 'record' not in url:
            relevance_score += 0.1
        
        # –®—Ç—Ä–∞—Ñ –∑–∞ –ø–æ–¥–æ–∑—Ä–∏—Ç–µ–ª—å–Ω—ã–µ URL (–Ω–∞–ø—Ä–∏–º–µ—Ä, —Å –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏)
        if url.count('?') > 1 or url.count('&') > 3:
            relevance_score -= 0.1
        
        # 5. –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –¥—É–±–ª–∏–∫–∞—Ç—ã –∏ –≤–∞—Ä–∏–∞—Ü–∏–∏
        # (–≠—Ç–æ –º–æ–∂–Ω–æ —Ä–∞—Å—à–∏—Ä–∏—Ç—å –¥–ª—è –¥–µ—Ç–µ–∫—Ü–∏–∏ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ ORCID)
        
        logger.info(f"INFO: ORCID {orcid_info['orcid']} - —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å: {relevance_score:.3f} (–ø–æ–∑–∏—Ü–∏—è: {position_bonus:.3f}, URL: {0.2 if orcid_info['is_complete_url'] else 0:.3f}, –¥–æ–º–µ–Ω: {domain_context})")
        
        return max(0.0, min(1.0, relevance_score))  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–∏–∞–ø–∞–∑–æ–Ω [0, 1]
    
    def _extract_domain_context(self, url: str) -> str:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –∫–æ–Ω—Ç–µ–∫—Å—Ç –¥–æ–º–µ–Ω–∞ –∏–∑ URL –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏"""
        try:
            from urllib.parse import urlparse
            parsed = urlparse(url)
            return parsed.netloc
        except:
            # Fallback –¥–ª—è –ø—Ä–æ—Å—Ç–æ–≥–æ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –¥–æ–º–µ–Ω–∞
            if '://' in url:
                domain_part = url.split('://')[1]
                return domain_part.split('/')[0]
            return url
    
    def _select_best_orcid(self, found_orcids: List[Dict[str, Any]]) -> Dict[str, Any]:
        """–í—ã–±–∏—Ä–∞–µ—Ç –Ω–∞–∏–±–æ–ª–µ–µ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–π ORCID –∏–∑ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö"""
        
        if not found_orcids:
            return None
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏ (—É–±—ã–≤–∞–Ω–∏–µ)
        sorted_orcids = sorted(found_orcids, key=lambda x: x['relevance_score'], reverse=True)
        
        best_orcid = sorted_orcids[0]
        
        logger.info(f"INFO: –ê–Ω–∞–ª–∏–∑ –≤—Å–µ—Ö –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö ORCID:")
        for i, orcid in enumerate(sorted_orcids):
            logger.info(f"INFO:   {i+1}. {orcid['orcid']} - —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å: {orcid['relevance_score']:.3f} (–ø–æ–∑–∏—Ü–∏—è –≤ —Å–ø–∏—Å–∫–µ: {orcid['position_in_list']+1})")
        
        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –ø—Ä–æ–≤–µ—Ä–∫–∏ –¥–ª—è —Ñ–∏–Ω–∞–ª—å–Ω–æ–≥–æ –≤—ã–±–æ—Ä–∞
        if len(sorted_orcids) > 1:
            # –ï—Å–ª–∏ —Ä–∞–∑–Ω–∏—Ü–∞ –≤ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏ –º–∏–Ω–∏–º–∞–ª—å–Ω–∞, –≤—ã–±–∏—Ä–∞–µ–º –ø–æ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–º –∫—Ä–∏—Ç–µ—Ä–∏—è–º
            score_diff = sorted_orcids[0]['relevance_score'] - sorted_orcids[1]['relevance_score']
            
            if score_diff < 0.1:  # –ï—Å–ª–∏ —Ä–∞–∑–Ω–∏—Ü–∞ –º–µ–Ω—å—à–µ 10%
                logger.info(f"INFO: –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è —Ä–∞–∑–Ω–∏—Ü–∞ –≤ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏ ({score_diff:.3f}), –ø—Ä–∏–º–µ–Ω—è–µ–º –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –∫—Ä–∏—Ç–µ—Ä–∏–∏")
                
                # –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç URL —Å 'orcid.org' –≤ –¥–æ–º–µ–Ω–µ
                orcid_domain_candidates = [o for o in sorted_orcids[:3] if 'orcid.org' in o['url'].lower()]
                if orcid_domain_candidates:
                    best_orcid = orcid_domain_candidates[0]
                    logger.info(f"INFO: –í—ã–±—Ä–∞–Ω ORCID —Å –æ—Ñ–∏—Ü–∏–∞–ª—å–Ω–æ–≥–æ –¥–æ–º–µ–Ω–∞: {best_orcid['orcid']}")
        
        return best_orcid

